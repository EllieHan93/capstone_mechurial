# -*- coding: utf-8 -*-
"""ìº¡ìŠ¤í†¤ë””ìì¸_RAG+string_Matching_v11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ez74dHalSmqhGxpPhXZiapYpwIQ8Yh5Q

## í™˜ê²½ì„¤ì •
"""

import logging
logging.basicConfig(level=logging.INFO)

from google.colab import drive
drive.mount('/content/drive')

# # !pip install gradio

# !pip uninstall -y langchain
# !pip install -U langchain

# !pip install -U langchain langchain-openai langchain-community

# !pip install -U langchain-text-splitters chromadb

# !pip install -U langchain-huggingface sentence-transformers

# !python3 --version

# !pip install konlpy

# !pip install kiwipiepy==0.22.1
# !pip install kiwipiepy_model==0.22.0

import os
import json
import pandas as pd
import numpy as np
from numpy.linalg import norm

from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.messages import AIMessage, HumanMessage
from langchain_huggingface import HuggingFaceEmbeddings

import re
from collections import Counter, defaultdict
from math import log
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

from IPython.display import display
import ipywidgets as widgets

from konlpy.tag import Okt

# from kiwipiepy import Kiwi

"""## ë°ì´í„° ê²½ë¡œ"""



VECTOR_DB_PATH = "/content/drive/MyDrive/CapStone/á„á…¢á†¸á„‰á…³á„á…©á†«/review_db_v13"
META_PATH = "/content/drive/MyDrive/CapStone/á„á…¢á†¸á„‰á…³á„á…©á†«/restaurant_metadata.json"

MENU_DATA_PATH = "/content/drive/MyDrive/CapStone/á„á…¢á†¸á„‰á…³á„á…©á†«/menu_data_final.csv"

"""## ê³µí†µí•¨ìˆ˜"""

okt_tokenizer = Okt()
# kiwi = Kiwi()

def normalize_tokens(text, is_use_v=True):
    text = re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", " ", text)
    okt_pos_tags = okt_tokenizer.pos(text, stem=True)

    base_tokens = []
    for w, p in okt_pos_tags:
        if p in ["Noun", "Adjective", "Verb"]:
            base_tokens.append(w)

    if is_use_v:
        valid_prefixes = ("N", "V", "VA", "XR")  # ë™ì‚¬ í¬í•¨
    else:
        valid_prefixes = ("N", "VA", "XR")       # ë™ì‚¬ ì œì™¸

    result = set()
    for w, p in okt_pos_tags:
        # í’ˆì‚¬ ì¡°ê±´
        if p == "Noun":
            result.add(w)
        elif p == "Adjective":
            result.add(w)
        elif is_use_v and p == "Verb":  # ë™ì‚¬ í¬í•¨ ì—¬ë¶€
            result.add(w)

    cleaned = set()
    for word in result:
        if word.endswith(("í•˜ë‹¤", "í•œ")):
            base = re.sub(r"(í•˜ë‹¤|í•œ)$", "", word)
            if len(base) >= 2:
                cleaned.add(base)
        cleaned.add(word)

    return list(cleaned)

def preprocess_query(text):
    stopwords = {"ì—ì„œ","ìœ¼ë¡œ","ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì˜","ë„","ì—",
                "ë¡œ","ê³¼","ì™€","ê³ ","ë“¤","ì¢€","ì²˜ëŸ¼","ë„","í•œ"}
    tokens = normalize_tokens(text)
    return [t for t in tokens if t not in stopwords]

def normalize_attr_dict(attr_dict):
    normalized = {}
    for attr, keywords in attr_dict.items():
        expanded = set()
        for w in keywords:
            expanded.update(normalize_tokens(w))
        normalized[attr] = list(expanded)
    return normalized

def apply_time_filter(df, tokens):
    lunch_kw = ["ì ì‹¬", "ëŸ°ì¹˜", "ë¸ŒëŸ°ì¹˜", "ì•„ì "]
    dinner_kw = ["ì €ë…", "ë””ë„ˆ", "ì•¼ì‹", "ì‹¬ì•¼", "ë°¤"]
    time_slot = None

    if any(t in tokens for t in lunch_kw):
        time_slot = "lunch"
    elif any(t in tokens for t in dinner_kw):
        time_slot = "dinner"

    if df is None:
        return None, time_slot

    if time_slot == "lunch":
        df_filtered = df[df.filter(regex="_lunch$").any(axis=1)]
        return df_filtered, time_slot

    elif time_slot == "dinner":
        df_filtered = df[df.filter(regex="_dinner$").any(axis=1)]
        return df_filtered, time_slot

    return df, time_slot

STATION_MAP = {
    "ì„±ìˆ˜": ["ì„±ìˆ˜", "ì„±ìˆ˜ì—­"],
    "ì„œìš¸ìˆ²": ["ì„œìš¸ìˆ²", "ì„œìš¸ìˆ²ì—­"],
    "ëšì„¬": ["ëšì„¬", "ëšì„¬ì—­"],
    "ìì–‘": ["ìì–‘", "ìì–‘ë™"],
    "ê±´ëŒ€ì…êµ¬": ["ê±´ëŒ€ì…êµ¬", "ê±´ëŒ€", "ê±´ëŒ€ê·¼ì²˜", "ê±´ëŒ€ì—­", "ê±´êµ­ëŒ€"],
    "ì–´ë¦°ì´ëŒ€ê³µì›": ["ì–´ë¦°ì´ëŒ€ê³µì›", "ì–´ë¦°ì´ëŒ€ê³µì›ì—­", "ì–´ëŒ€ê³µ", "ëŒ€ê³µì›"],
}

def extract_station_from_keywords(keywords, station_map=STATION_MAP):
    matched = set()
    keywords_str = " ".join(keywords)

    for std, syns in station_map.items():
        for syn in syns:
            if syn in keywords or syn in keywords_str:
                matched.add(std)
                break

    return list(matched)

def extract_query_intent(query):
    tokens = normalize_tokens(query)

    stations = extract_station_from_keywords(tokens)
    _, time_slot = apply_time_filter(None, tokens)

    return {
        "tokens": tokens,
        "stations": stations,
        "time_slot": time_slot,
    }

"""## ì‹¤ì‹œê°„ ë°ì´í„°"""



import requests
import json
from datetime import datetime, timedelta
import math
import xml.etree.ElementTree as ET
from math import radians, cos, sin, asin, sqrt

# SERVICE_KEYëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ì–´ì˜µë‹ˆë‹¤
# export SERVICE_KEY="your-service-key" ë˜ëŠ” .env íŒŒì¼ ì‚¬ìš©
SERVICE_KEY = os.environ.get("SERVICE_KEY")
if not SERVICE_KEY:
    raise ValueError("SERVICE_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
# REALTIME_CITY_DATA_URL = f'http://openapi.seoul.go.kr:8088/{SERVICE_KEY}/json/citydata/1/1000/'
REALTIME_CITY_DATA_URL = f'http://openapi.seoul.go.kr:8088/{SERVICE_KEY}/xml/citydata/1/1000/'
ATTR_LOCATION_KEY ={
    "ì„±ìˆ˜" : "ì„±ìˆ˜ì¹´í˜ê±°ë¦¬",
    "ì„œìš¸ìˆ²" : "ì„œìš¸ìˆ²ê³µì›",
    "ëšì„¬" : "ëšì„¬ì—­",
    "ê±´ëŒ€ì…êµ¬" : "ê±´ëŒ€ì…êµ¬ì—­",
    "ì–´ë¦°ì´ëŒ€ê³µì›" : "ì–´ë¦°ì´ëŒ€ê³µì›"
}

def get_station_to_location(station):
    return ATTR_LOCATION_KEY.get(station)

def get_city_datas():
    city_datas = {}
    stations = INTENT_STATE["stations"]
    for st in stations:
          loc = get_station_to_location(st)
          if not loc:
              continue

          url = REALTIME_CITY_DATA_URL + loc
          print(f"city data url : {url}")
          response = requests.get(url)

          if response.status_code != 200:
              continue

          city_datas[loc] = response.content.decode("utf-8")

    return city_datas

import pandas as pd
import xml.etree.ElementTree as ET

def xml_to_dataframe(xml_data, keyword):
    import xml.etree.ElementTree as ET
    if xml_data is None:
        return pd.DataFrame()

    if isinstance(xml_data, ET.Element):
        root = xml_data
    else:
        root = ET.fromstring(xml_data)

    rows = []
    for elem in root.findall(f".//{keyword}"):
        if len(list(elem)) == 0:
            continue

        row = {child.tag: child.text for child in elem}
        rows.append(row)

    df = pd.DataFrame(rows)

    df = df.dropna(how='all')
    return df.reset_index(drop=True)

"""### ì¸êµ¬ ë°ì´í„°"""

def get_pop_status_data(city_data):
    return xml_to_dataframe(city_data, "LIVE_PPLTN_STTS")

"""### ë‚ ì”¨ ë°ì´í„°"""

def get_weather_data(city_data):
    return xml_to_dataframe(city_data, "WEATHER_STTS")

"""### ì£¼ì°¨ ë°ì´í„°"""

def haversine(target_lat, target_lon, actual_lat, actual_lon):
  r = 6371

  dlat = radians(actual_lat-target_lat)
  dlon = radians(actual_lon-target_lon)

  a = sin(dlat / 2)**2 + cos(radians(target_lat)) * cos(radians(actual_lat)) * sin(dlon / 2)**2
  c = 2 * asin(sqrt(a))

  return r * c * 1000

def get_parking_info(target_df, city_data):
    print(f"city_data : {city_data}")
    print(f"target_df : {target_df}")
    parking_df = xml_to_dataframe(city_data, "PRK_STTS")
    if parking_df.empty:
        print("ì£¼ì°¨ì¥ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    print(f"parking_df : {parking_df}")

    parking_df['LAT'] = parking_df['LAT'].astype(float)
    parking_df['LNG'] = parking_df['LNG'].astype(float)

    parking_df['distance'] = parking_df.apply(
        lambda row: haversine(target_df.iloc[0]['lon'], target_df.iloc[0]['lat'], row['LAT'], row['LNG']),
        axis=1
        )

    return parking_df.sort_values(by='distance', ascending=True).head(5)

"""ì§€ì—­ëª… ê¸°ë°˜ ì£¼ì°¨ì¥ ê±°ë¦¬ ê³„ì‚°"""
STATION_CENTER_COORDINATES = {
    "ì„±ìˆ˜": (37.5446, 127.0551),
    "ì„œìš¸ìˆ²": (37.5434, 127.0447),
    "ëšì„¬": (37.5472, 127.0474),
    "ê±´ëŒ€ì…êµ¬": (37.5403, 127.0694),
    "ì–´ë¦°ì´ëŒ€ê³µì›": (37.5480, 127.0747)
}

def get_parking_info_by_location(location_name, city_data):

    if city_data is None:
        return pd.DataFrame()

    # PRK_STTS íŒŒì‹±
    parking_df = xml_to_dataframe(city_data, "PRK_STTS")
    if parking_df.empty:
        return parking_df

    # ì¢Œí‘œ íƒ€ì… ë³€í™˜
    parking_df["LAT"] = parking_df["LAT"].astype(float)
    parking_df["LNG"] = parking_df["LNG"].astype(float)

    # ì§€ì—­ ì¢Œí‘œ ì—†ëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
    if location_name not in STATION_CENTER_COORDINATES:
        return parking_df

    # ì¤‘ì‹¬ ì¢Œí‘œ
    center_lat, center_lon = STATION_CENTER_COORDINATES[location_name]

    # ê±°ë¦¬ ê³„ì‚°
    parking_df["distance"] = parking_df.apply(
        lambda row: haversine(
            center_lon, center_lat,
            row["LAT"], row["LNG"]
        ),
        axis=1
    )

    return parking_df.sort_values(by="distance")

"""### í…ŒìŠ¤íŠ¸"""

# temp_data = get_city_datas()
# print(temp_data)
# temp_prk = get_prk_data(temp_data["ì„±ìˆ˜ì¹´í˜ê±°ë¦¬"])
# print(temp_prk.head(3))



"""## RAG"""

# !apt-get install openjdk-11-jdk -y

EMBEDDING_MODEL = "BM-K/KoSimCSE-roberta-multitask"
LLM_MODEL = "gpt-4o-mini"

# API í‚¤ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ì–´ì˜µë‹ˆë‹¤
# export OPENAI_API_KEY="your-api-key" ë˜ëŠ” .env íŒŒì¼ ì‚¬ìš©
if "OPENAI_API_KEY" not in os.environ:
    raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def prepare_rag_env(
    vector_db_path: str,
    metadata_path: str,
    collection_name: str = "review_collection",
    embedding_model: str = "BM-K/KoSimCSE-roberta-multitask"
):

    vector_db = Chroma(
        collection_name=collection_name,
        embedding_function=OpenAIEmbeddings(model=embedding_model),
        persist_directory=vector_db_path
    )
    count = vector_db._collection.count()
    print(f"ë²¡í„°DB ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {count}ê°œ")


    if not os.path.exists(metadata_path):
        raise FileNotFoundError(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ: {metadata_path}")

    with open(metadata_path, "r", encoding="utf-8") as f:
      restaurant_metadata_list = json.load(f)

    restaurant_metadata_df = pd.DataFrame(restaurant_metadata_list)
    print(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(restaurant_metadata_df)}ê°œ")

    vector_db_items = vector_db.get(include=["metadatas", "documents"])

    vector_db_metadata_df = pd.DataFrame(vector_db_items["metadatas"])
    vector_db_metadata_df["page_content"] = vector_db_items["documents"]

    print(f"DBì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ: {len(vector_db_metadata_df)}ê°œ")


    merged_restaurant_review_df = pd.merge(
        vector_db_metadata_df,
        restaurant_metadata_df,
        on="restaurant_id",
        how="left",
        suffixes=("_db", "_json")
    )


    print(f"ë³‘í•© ì™„ë£Œ: {len(merged_restaurant_review_df)}ê°œ ë§¤ì¹­")
    display(merged_restaurant_review_df[["restaurant_id", "restaurant_name_db", "review_no"]].head(5))

    unmatched_restaurants = merged_restaurant_review_df[merged_restaurant_review_df["restaurant_name_json"].isna()]
    print(f"ë§¤ì¹­ ì•ˆ ëœ ì‹ë‹¹ ìˆ˜: {len(unmatched_restaurants)}")

    before_count = len(merged_restaurant_review_df)
    merged_restaurant_review_df = merged_restaurant_review_df[merged_restaurant_review_df["restaurant_name_json"].notna()].reset_index(drop=True)
    after_count = len(merged_restaurant_review_df)
    print(f" ë§¤ì¹­ ì•ˆ ëœ ì‹ë‹¹ {before_count - after_count}ê°œ ì‚­ì œ ì™„ë£Œ")
    print(f"ë‚¨ì€ ë°ì´í„° ìˆ˜: {after_count}ê°œ")

    duplicate_columns_to_drop = ["restaurant_name_json", "category_1_json", "category_2_json", "station_json"]
    duplicate_columns_to_drop = [c for c in duplicate_columns_to_drop if c in merged_restaurant_review_df.columns]
    merged_restaurant_review_df = merged_restaurant_review_df.drop(columns = duplicate_columns_to_drop)
    print(merged_restaurant_review_df)

    print(f"ìµœì¢… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {len(merged_restaurant_review_df)}ê°œ")

    print("vector_db_metadata_df ì»¬ëŸ¼ ëª©ë¡:\n", vector_db_metadata_df.columns.tolist())
    print("\n restaurant_metadata_df ì»¬ëŸ¼ ëª©ë¡:\n", restaurant_metadata_df.columns.tolist())
    print("\n merged_restaurant_review_df ì»¬ëŸ¼ ëª©ë¡:\n", merged_restaurant_review_df.columns.tolist())


    return vector_db, merged_restaurant_review_df

vector_db, merged_restaurant_review_df = prepare_rag_env(VECTOR_DB_PATH, RESTAURANT_METADATA_PATH)
embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

merged_restaurant_review_df.info()

merged_restaurant_review_df.head(3)

cols = ['restaurant_id'] + [c for c in merged_restaurant_review_df.columns if c != 'restaurant_id']
print(merged_restaurant_review_df[cols].sort_values(by='restaurant_id'))

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["PATH"] += os.pathsep + os.path.join(os.environ["JAVA_HOME"], "bin")

"""### 1ì°¨ í•„í„°ë§"""

def apply_station_filter(df, keywords):
    if "station_db" not in df.columns:
        return df

    matched_station_names = INTENT_STATE["stations"]

    if not matched_station_names:
        return df

    station_filter_condition = pd.Series(False, index=df.index)
    for station_name in matched_station_names:
        station_filter_condition |= df["station_db"].astype(str).str.contains(station_name, na=False)

    return df[station_filter_condition]


def apply_category_filter(df, keywords):
    if "category_1_db" not in df.columns:
        return df

    keywords_str = " ".join(keywords)

    category_map = {
        "í•œì‹": ["í•œì‹", "ë°±ë°˜", "êµ­ë°¥", "ì‚¼ê²¹ì‚´", "ëª©ì‚´", "ì¹¼êµ­ìˆ˜", "ë³´ìŒˆ", "í•œì •ì‹"],
        "ì¼ì‹": ["ì¼ì‹", "ìŠ¤ì‹œ", "ì´ˆë°¥", "ë¼ë©˜", "ë®ë°¥", "ê°€ì¸ ë™", "ì˜¤ë§ˆì¹´ì„¸", "ì˜¤ë‹ˆê¸°ë¦¬", "ê·œë™"],
        "ì¤‘ì‹": ["ì¤‘ì‹", "ì§œì¥", "ì§¬ë½•", "íƒ•ìˆ˜ìœ¡", "í› ê¶ˆ", "ë§ˆë¼ìƒ¹ê¶ˆ", "ì¤‘í™”ìš”ë¦¬", "ë§ˆë¼íƒ•", "ì–‘ê¼¬ì¹˜"],
        "ì–‘ì‹": ["ì–‘ì‹", "íŒŒìŠ¤íƒ€", "ìŠ¤í…Œì´í¬", "ë¸ŒëŸ°ì¹˜"],
        "ìˆ ì§‘": ["ìˆ ì§‘", "ë°”", "ì´ìì¹´ì•¼", "í", "í¬ì°¨", "ì†Œì£¼", "ë§¥ì£¼", "í¬ì¥ë§ˆì°¨", "ì•ˆì£¼"],
        "ê°„ì‹": ["ê°„ì‹", "ì¹´í˜", "ê¹Œí˜", "ë² ì´ì»¤ë¦¬", "ë¹µì§‘", "ë² ì´ê¸€", "ë¸ŒëŸ°ì¹˜", "ë””ì €íŠ¸"],
        "ë¶„ì‹": ["ë¶„ì‹", "ë–¡ë³¶ì´", "ì¦‰ë–¡", "ê¹€ë°¥", "ë¼ë©´"],
    }

    simple_categories = [
        "êµ¬ë‚´ì‹ë‹¹", "ì¹˜í‚¨", "íŒ¨ìŠ¤íŠ¸í‘¸ë“œ", "í“¨ì „ìš”ë¦¬", "ë„ì‹œë½",
        "ì•„ì‹œì•„ìŒì‹", "ìƒ¤ë¸Œìƒ¤ë¸Œ", "ë·”í˜", "ìƒëŸ¬ë“œ", "íŒ¨ë°€ë¦¬ë ˆìŠ¤í† ë‘"
    ]

    category_keyword_used = (
        any(any(syn in keywords_str for syn in syns) for syns in category_map.values())
        or any(cat in keywords_str for cat in simple_categories)
    )

    if not category_keyword_used:
      return df

    category_filter_condition = pd.Series(True, index=df.index)
    is_category_matched = False


    for category_name, synonyms in category_map.items():
        if any(synonym in keywords_str for synonym in synonyms):
            category_filter_condition &= df["category_1_db"].str.contains(category_name, case=False, na=False)
            is_category_matched = True
            break

    if not is_category_matched:
        for category_name in simple_categories:
            if category_name in keywords_str:
                category_filter_condition &= df["category_1_db"].str.contains(category_name, case=False, na=False)
                is_category_matched = True
                break
    return df[category_filter_condition]



def apply_attribute_filter(df, keywords):
    attr_map = {
        "service_score": ["ì¹œì ˆ", "ì‘ëŒ€", "ì„œë¹„ìŠ¤", "ë¹ ë¥¸"],
        "ambience_score": ["ë¶„ìœ„ê¸°", "ì¡°ìš©", "ê°ì„±", "ë°ì´íŠ¸", "ì•„ëŠ‘", "ë¡œë§¨í‹±", "ê¸°ë…ì¼"],
        "price_score": ["ì €ë ´", "ê°€ì„±ë¹„", "í•©ë¦¬ì ", "í˜œì"],
        "taste_score": ["ë§›ìˆ", "ë§›ì§‘", "í›Œë¥­"],
    }

    for col, syns in attr_map.items():
        if col in df.columns and any(s in keywords for s in syns):
            df = df[df[col] >= 0.5]

    return df


def build_filters(query, verbose=False):
    # keywords = normalize_tokens(query)
    keywords = INTENT_STATE["tokens"]
    total_review_count = len(merged_restaurant_review_df)
    filtered_reviews_df = merged_restaurant_review_df.copy()

    before_station_filter = len(filtered_reviews_df)
    filtered_reviews_df = apply_station_filter(filtered_reviews_df, keywords)
    if verbose:
        print(f"ì—­ í•„í„°: {before_station_filter} â†’ {len(filtered_reviews_df)}")

    before_time_filter = len(filtered_reviews_df)
    filtered_reviews_df, time_slot = apply_time_filter(filtered_reviews_df, keywords)
    if verbose:
        print(f"ì‹œê°„ëŒ€ í•„í„°: {before_time_filter} â†’ {len(filtered_reviews_df)}")

    before_category_filter = len(filtered_reviews_df)
    filtered_reviews_df = apply_category_filter(filtered_reviews_df, keywords)
    if verbose:
        print(f"ì¹´í…Œê³ ë¦¬ í•„í„°: {before_category_filter} â†’ {len(filtered_reviews_df)}")

    before_attribute_filter = len(filtered_reviews_df)
    filtered_reviews_df = apply_attribute_filter(filtered_reviews_df, keywords)
    if verbose:
        print(f"ì†ì„± í•„í„°: {before_attribute_filter} â†’ {len(filtered_reviews_df)}")

    if verbose:
        unique_restaurant_count = filtered_reviews_df['restaurant_name_db'].nunique()
        print(f"ìµœì¢…: {total_review_count} â†’ {len(filtered_reviews_df)} (ë¦¬ë·°/ì‹ë‹¹ {unique_restaurant_count}ê°œ ë‚¨ìŒ)")

    return filtered_reviews_df



"""### 2ì°¨ ì„ë² ë”©

#### ê¸°ë³¸ Retrieve
"""

def calc_review_similarity(vector_subset_dict, query_embedding):
    review_embedding_vectors = np.array(vector_subset_dict["embeddings"])
    similarity_scores = np.dot(review_embedding_vectors, query_embedding) / (
        norm(review_embedding_vectors, axis=1) * norm(query_embedding)
    )

    similarity_df = pd.DataFrame({
        "review_no": [metadata["review_no"] for metadata in vector_subset_dict["metadatas"]],
        "restaurant_id": [metadata["restaurant_id"] for metadata in vector_subset_dict["metadatas"]],
        "restaurant_name": [metadata["restaurant_name"] for metadata in vector_subset_dict["metadatas"]],
        "date": [metadata["date"] for metadata in vector_subset_dict["metadatas"]],
        "similarity": similarity_scores,
    })

    return similarity_df;

def retrieve_similar_reviews(query,
                             filtered_reviews_df,
                             lambda_weight=0.0001):

    query_embedding = embedding_model.embed_query(query)

    filtered_review_ids = filtered_reviews_df["review_no"].astype(int).tolist()

    vector_subset_dict = vector_db._collection.get(
        where={"review_no": {"$in": filtered_review_ids}},
        include=["embeddings", "metadatas"]
    )

    # print(f"ë²¡í„°DB ì„ë² ë”© ë¡œë“œ ê°œìˆ˜: {len(vector_subset_dict['embeddings'])}")

    if len(vector_subset_dict["embeddings"]) == 0:
        return pd.DataFrame(), pd.DataFrame()

    similarity_result_df = calc_review_similarity(vector_subset_dict, query_embedding)
    # print(f"ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼ : {similarity_result_df.head(5)}")

    return similarity_result_df

"""#### Time Weight"""

def calc_timeWeighted_date(similarity_df, lambda_weight=0.0009):
    similarity_df["date"] = pd.to_datetime(similarity_df["date"], format="mixed")
    latest_review_date = similarity_df["date"].max()
    similarity_df["days_since"] = (latest_review_date - similarity_df["date"]).dt.days

    similarity_df["time_weight"] = np.exp(-lambda_weight * similarity_df["days_since"])
    similarity_df["weighted_similarity"] = similarity_df["similarity"] * similarity_df["time_weight"]

    return similarity_df

def time_weight_with_retrieve_similar_reviews(query,
                             filtered_reviews_df,
                             lambda_weight=0.0009):

    query_embedding = embedding_model.embed_query(query)

    filtered_review_ids = filtered_reviews_df["review_no"].astype(int).tolist()

    vector_subset_dict = vector_db._collection.get(
        where={"review_no": {"$in": filtered_review_ids}},
        include=["embeddings", "metadatas"]
    )

    print(f"ë²¡í„°DB ì„ë² ë”© ë¡œë“œ ê°œìˆ˜: {len(vector_subset_dict['embeddings'])}")

    if len(vector_subset_dict["embeddings"]) == 0:
        return pd.DataFrame(), pd.DataFrame()

    similarity_result_df = calc_review_similarity(vector_subset_dict, query_embedding)
    # print(f"ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼ : {similarity_result_df.head(5)}")

    time_weighted_similarity_df = calc_timeWeighted_date(similarity_result_df, lambda_weight)
    # print(f"ìµœì‹  ë¦¬ë·° ê°€ì¤‘ì¹˜ ë¶€ì—¬ ê²°ê³¼ : {time_weighted_similarity_df.head(5)}")

    return time_weighted_similarity_df

"""
#### Result"""

def select_topk_groupby_restaurants(similarity_df, top_k=3, reviews_per_restaurant=10, sort_column="weighted_similarity"):
  top_restaurants_df = (
        similarity_df.sort_values(["restaurant_id", sort_column], ascending=[True, False])
        .groupby("restaurant_id")
        .head(reviews_per_restaurant)
        .groupby(["restaurant_id", "restaurant_name"], as_index=False)[sort_column]
        .mean()
        .sort_values(sort_column, ascending=False)
    )

  top_restaurants_df = top_restaurants_df.head(top_k)

  return top_restaurants_df

"""#### í•„í„°ë§ + ì„ë² ë”©"""

def rag_recommend_reviews(query):
    filtered_df = build_filters(query,True)

    raw_sim_df = retrieve_similar_reviews(query, filtered_df)
    retrieve_topk_df = select_topk_groupby_restaurants(raw_sim_df, sort_column="similarity")
    print(f"raw similarity top-k ê²°ê³¼ : {retrieve_topk_df.head(5)}")

    return retrieve_topk_df

def rag_recommend_reviews_with_time_weight(query):
    filtered_df = build_filters(query,True)

    time_weighted_df = time_weight_with_retrieve_similar_reviews(query, filtered_df)
    time_weight_topk_df = select_topk_groupby_restaurants(time_weighted_df, sort_column="weighted_similarity")
    print(f"time-weight top-k ê²°ê³¼ : {time_weight_topk_df.head(5)}")

    return time_weighted_df, time_weight_topk_df

def get_rest_metadata(restaurant_id):
    restaurant_row = merged_restaurant_review_df[merged_restaurant_review_df["restaurant_id"] == restaurant_id].head(1)
    if restaurant_row.empty:
        return {}
    return restaurant_row.iloc[0].to_dict()


def get_top_reviews_from_df(similarity_df, restaurant_id, top_k):

    return (
        similarity_df[similarity_df["restaurant_id"] == restaurant_id]
        .nlargest(top_k, "weighted_similarity")
    )


def get_reviews_from_vector_db(review_id_list):
    vector_db_result = vector_db._collection.get(
        where={"review_no": {"$in": review_id_list}},
        include=["documents", "metadatas"]
    )

    return list(zip(vector_db_result.get("documents", []),
                    vector_db_result.get("metadatas", [])))


def build_review_json(top_reviews_df, review_document_metadata_pairs):
    reviews_json_list = []
    for document_text, review_metadata in review_document_metadata_pairs:
        review_number = review_metadata.get("review_no")
        similarity_values = top_reviews_df[top_reviews_df["review_no"] == review_number]["weighted_similarity"].values
        weighted_similarity_score = float(similarity_values[0]) if len(similarity_values) else 0.0

        reviews_json_list.append({
            "review_no": review_number,
            "date": str(review_metadata.get("date")),
            "similarity": weighted_similarity_score,
            "text": document_text
        })

    print(f"reviews : {reviews_json_list}")
    return reviews_json_list


def retrieve_result_to_json(similarity_df, top_restaurants_df, top_k=3):
    restaurant_context_list = []

    for _, restaurant_row in top_restaurants_df.iterrows():
        restaurant_id = restaurant_row["restaurant_id"]
        restaurant_name = restaurant_row["restaurant_name"]

        restaurant_metadata = get_rest_metadata(restaurant_id)
        top_reviews_df = get_top_reviews_from_df(similarity_df, restaurant_id, top_k)

        review_document_metadata_pairs = get_reviews_from_vector_db(
            top_reviews_df["review_no"].astype(int).tolist()
        )
        reviews_json_list = build_review_json(top_reviews_df, review_document_metadata_pairs)

        restaurant_context_list.append({
            "restaurant_id": restaurant_id,
            "restaurant_name": restaurant_name,
            "meta": restaurant_metadata,
            "top_reviews": reviews_json_list
        })

    # print(f"retrieve_result_to_json : {top_restaurants_df.head(5)}")

    llm_context_data = {"restaurants": restaurant_context_list}
    print(f"restaurants : {llm_context_data}")


    return llm_context_data

"""### MMR"""

def select_reviews_mmr(
    query,
    # query_embedding,
    # review_embedding_vectors,
    # review_metadatas,
    lambda_mult=0.5,
    top_k=3,
    verbose=True
):

    filtered_reviews_df = build_filters(query)
    filtered_review_ids = filtered_reviews_df["review_no"].astype(int).tolist()
    query_embedding = embedding_model.embed_query(query)
    vector_subset_dict = vector_db._collection.get(
        where={"review_no": {"$in": filtered_review_ids}},
        include=["embeddings", "metadatas"]
    )
    review_embedding_vectors = np.array(vector_subset_dict["embeddings"])
    review_metadatas = vector_subset_dict["metadatas"]

    similarity_to_query = cosine_similarity([query_embedding], review_embedding_vectors)[0]
    similarity_between_reviews = cosine_similarity(review_embedding_vectors)

    #langchainì˜ MMR ìˆ˜ì‹ì„ ì§ì ‘ ë°˜ì˜(ì¿¼ë¦¬ì™€ì˜ ìœ ì‚¬ë„ëŠ” ë†’ê²Œ, ì´ë¯¸ ì„ íƒëœ ë¦¬ë·°ë“¤ê³¼ ìœ ì‚¬ë„ëŠ” ë‚®ê²Œ ì¡°ì •)
    selected_indices, remaining_indices = [], list(range(len(review_embedding_vectors)))
    while len(selected_indices) < top_k and remaining_indices:
        mmr_scores = [
            lambda_mult * similarity_to_query[idx]
            - (1 - lambda_mult) * max([similarity_between_reviews[idx][selected_idx] for selected_idx in selected_indices] or [0])
            for idx in remaining_indices
        ]
        best_mmr_idx = remaining_indices[np.argmax(mmr_scores)]
        selected_indices.append(best_mmr_idx)
        remaining_indices.remove(best_mmr_idx)

    selected_review_metadatas = [review_metadatas[idx] for idx in selected_indices]
    selected_similarity_scores = similarity_to_query[selected_indices]

    mmr_result_df = pd.DataFrame({
        "review_no": [metadata.get("review_no") for metadata in selected_review_metadatas],
        "restaurant_id": [metadata.get("restaurant_id") for metadata in selected_review_metadatas],
        "restaurant_name": [metadata.get("restaurant_name") for metadata in selected_review_metadatas],
        "date": [metadata.get("date") for metadata in selected_review_metadatas],
        "similarity": selected_similarity_scores,
    })

    if verbose:
        print(f"\n MMR ê¸°ë°˜ ë¦¬ë·° Top-{top_k} ì„ ì • ì™„ë£Œ:")
        for _, restaurant_row in mmr_result_df.iterrows():
            print(f"- {restaurant_row['restaurant_name']} ({restaurant_row['similarity']:.3f})")

    return mmr_result_df

"""## ë©”ë‰´ê¸°ë°˜ ì¶”ì¶œ

### ë©”ë‰´ + ì—­ ì •ë³´ ë³‘í•©
"""

# ##ê° ë©”ë‰´ íƒ€ì´í‹€, tokenize

# df = pd.read_csv('/content/drive/MyDrive/Ehwa/á„á…¢á†¸á„‰á…³á„á…©á†«/menu_with_store.csv')
# menu_store_df  = df.copy()
# menu_store_df["title_tokens"] = menu_store_df["menu_title"].astype(str).apply(normalize_tokens)
# menu_store_df.head()

# menu_store_token = pd.read_csv('/content/drive/MyDrive/Ehwa/á„á…¢á†¸á„‰á…³á„á…©á†«/menu_with_token.csv')
# menu_store_token.head()
# menu_store_token.info()

# menu_store_token.head(5)

# station_map = merged_df[['restaurant_id', 'station_db']].drop_duplicates(subset=['restaurant_id'])
# station_map.head(10)

# # ê° ë©”ë‰´íƒ€ì´í‹€ ì¥ì†Œ ì‚½ì…
# menu_store_token = menu_store_token.merge(
#     station_map[['restaurant_id','station_db']],
#     on='restaurant_id',
#     how='left'
# )
# menu_store_token = menu_store_token.rename(columns={'station_db': 'station'})
# menu_store_token.head(10)
# print(menu_store_token.sort_values(by='restaurant_id'))

# missing_ids = menu_store_token.loc[
#     menu_store_token['station'].isna(),
#     'restaurant_id'
# ].unique()

# print("station NaN restaurant_id ê°œìˆ˜:", len(missing_ids))
# print("ì¼ë¶€ í™•ì¸:", missing_ids[:10])

# nan_ratio = menu_store_token['station'].isna().mean()
# print(f"station NaN ë¹„ìœ¨: {nan_ratio:.2%}")

# menu_store_token[menu_store_token['station'].isna()].head()

# menu_store_token.to_csv('/content/drive/MyDrive/Ehwa/á„á…¢á†¸á„‰á…³á„á…©á†«/menu_data_final.csv', index=False, encoding='utf-8-sig')



"""###string matching"""

menu_raw_df = pd.read_csv(MENU_DATA_PATH)
menu_store_df  = menu_raw_df.copy()


TEXT_COLS = ["menu_title"]

ATTRIBUTE_COLS = ["spicy", "vegan", "meat", "seafood",
                  "diet", "dessert", "healthy",
                  "bakery", "alcohol_pair", "beverage_alcohol",
                  "soup", "cold", "hot"]

ATTR_TRANSLATIONS = {
    "spicy": ["ë§¤ìš´", "ì–¼í°í•œ", "ì¹¼ì¹¼í•œ", "ìê·¹ì ì¸", "ë§¤ì½¤í•œ"],
    "vegan": ["ë¹„ê±´", "ì±„ì‹", "ì±„ì†Œë§Œ", "ê³ ê¸°ì—†ëŠ”"],
    "meat": ["ê³ ê¸°", "ì‚¼ê²¹ì‚´", "ì†Œê³ ê¸°", "ë¼ì§€ê³ ê¸°", "ë‹­ê³ ê¸°", "ìœ¡ë¥˜"],
    "seafood": ["í•´ì‚°ë¬¼", "ìƒì„ ", "ìƒˆìš°", "ì¡°ê°œ", "ì˜¤ì§•ì–´", "íšŒ"],
    "diet": ["ë‹¤ì´ì–´íŠ¸", "ì €ì¹¼ë¡œë¦¬", "ê°€ë³ê²Œ", "ì‚´ì•ˆì°ŒëŠ”"],
    "dessert": ["ë””ì €íŠ¸", "ì¼€ì´í¬", "ë¹™ìˆ˜", "ì•„ì´ìŠ¤í¬ë¦¼", "ë‹¬ë‹¬í•œ"],
    "healthy": ["ê±´ê°•ì‹", "ì˜ì–‘ì‹", "ë‹´ë°±í•œ", "ì €ì—¼ì‹"],
    "bakery": ["ë² ì´ì»¤ë¦¬", "ë¹µ", "í¬ë¡œì™€ìƒ", "ë°”ê²ŒíŠ¸"],
    "alcohol_pair": ["ì•ˆì£¼", "ìˆ ì•ˆì£¼", "ë§¥ì£¼ì•ˆì£¼", "ì†Œì£¼ì•ˆì£¼"],
    "beverage_alcohol": ["ìŒë£Œ", "ìˆ ", "ë§¥ì£¼", "ì†Œì£¼", "ì¹µí…Œì¼", "ì»¤í”¼"],
    "soup": ["êµ­ë¬¼", "ì°Œê°œ", "íƒ•", "êµ­"],
    "cold": ["ì°¨ê°€ìš´", "ì‹œì›í•œ", "ëƒ‰ë©´", "ëƒ‰êµ­ìˆ˜"],
    "hot": ["ë”°ëœ»í•œ", "ëœ¨ê±°ìš´", "ë°ìš´", "ì˜¨"]
}


TIME_ATTR_WEIGHTS = {
  "lunch": {
        # ì ì‹¬ ì£¼ë ¥: ë“ ë“ í•œ ì‹ì‚¬, ê±´ê°•
        "rice": 2.5,
        "noodle": 2.5,
        "healthy": 2.0,
        "vegan": 2.0,
        "diet": 2.0,
        "meat": 1.5,      # ì ì‹¬ ê³ ê¸°ë„ ê´œì°®ìŒ

        # ì ì‹¬ ë¹„ì„ í˜¸ (íŒ¨ë„í‹°)
        "alcohol_pair": 0.5,
        "beverage_alcohol": 0.5,
        "spicy": 1.0,     # ë„ˆë¬´ ë§¤ìš´ê±´ ì ì‹¬ì— ë¶€ë‹´ì¼ ìˆ˜ ìˆìŒ (ì¤‘ë¦½)
    },

    "dinner": {
        # ì €ë… ì£¼ë ¥: ìˆ , íšŒì‹, ê±°í•œ ìš”ë¦¬
        "alcohol_pair": 3.0,      # ìˆ  ì•ˆì£¼ (ê°•ë ¥)
        "beverage_alcohol": 3.0,  # ì£¼ë¥˜ (ê°•ë ¥)
        "meat": 2.5,              # ê³ ê¸°
        "seafood": 2.5,           # í•´ì‚°ë¬¼
        "spicy": 2.5,             # ë§¤ìš´ë§›
        "hot": 2.0,               # ëœ¨ê±°ìš´ ìš”ë¦¬
        "soup": 2.0,              # êµ­ë¬¼

        # ì €ë… ë¹„ì„ í˜¸
        "bakery": 0.6,            # ë¹µì€ ì‹ì‚¬ë¡œ ë¶€ì¡±
        "dessert": 0.6,
        "diet": 0.8,              # ì €ë… íšŒì‹ë•Œ ë‹¤ì´ì–´íŠ¸ëŠ” ì ì‹œ...
    },
}


ATTR_TRANSLATIONS_NORM = normalize_attr_dict(ATTR_TRANSLATIONS)


def detect_attributes(attr_dict, threshold =0):
    attr_scores = {attr: 0 for attr in attr_dict}

    for attr, keywords in attr_dict.items():
        for word in INTENT_STATE["tokens"]:
            if word in keywords:
                attr_scores[attr] += 1

    detected = [attr for attr, score in attr_scores.items() if score > threshold]
    detected = sorted(detected, key=lambda x: attr_scores[x], reverse=True)

    return detected if detected else None

def preprocess_dataframe(df, query):
    if "ë·”í˜" not in query:
      df = df[~(
          (df["category_1"].str.contains("ë·”í˜", na=False)) |
          (df["menu_title"].str.contains("ë·”í˜", na=False))
      )]
    if "ì„¸íŠ¸" not in query:
      df = df[~(
          (df["menu_title"].str.contains("ì •ì‹", na=False))|
          (df["menu_title"].str.contains("ì„¸íŠ¸", na=False))
      )]
    return df



def score_menus(df, rating_weight=0.2):
    detected_attrs = detect_attributes(ATTR_TRANSLATIONS_NORM,threshold=0)
    print("ê°ì§€ëœ ì†ì„±:", detected_attrs)
    print("ê°ì§€ëœ ì‹œê°„ëŒ€:", INTENT_STATE["time_slot"])
    df = df.drop_duplicates(subset=["restaurant_id"]).copy()

    def calc_simple_score(row):
      if not detected_attrs: return 0.0
      total = sum(row.get(attr, 0) * 1.0 for attr in detected_attrs)
      return total / len(detected_attrs)

    def calc_weighted_score(row):
      if not detected_attrs: return 0.0
      total = 0.0
      for attr in detected_attrs:
          base_val = row.get(attr, 0)
          w = 1.0
          if INTENT_STATE.get("time_slot") in TIME_ATTR_WEIGHTS:
              if attr in TIME_ATTR_WEIGHTS[INTENT_STATE["time_slot"]]:
                  w = TIME_ATTR_WEIGHTS[INTENT_STATE["time_slot"]][attr]
      total += base_val * w
      return total / len(detected_attrs)


    if "rating_bayes" not in df.columns:
        df["rating_bayes"] = 2.0

    df["temp_simple"] = df.apply(calc_simple_score, axis=1)
    df["temp_time_slot"] = df.apply(calc_weighted_score, axis=1)
    df["match_score"] = df["temp_time_slot"]

    df["rating_bayes"] = pd.to_numeric(df["rating_bayes"], errors="coerce").fillna(0)
    df["match_score"] = pd.to_numeric(df["match_score"], errors="coerce").fillna(0)




    # print(f"ë² ì´ì§€ì•ˆ 30% {}")
    df["ë² ì´ì§€ì•ˆ"] = rating_weight * df["rating_bayes"]

    df["ì†ì„±ê°€ì¤‘ì¹˜ì ìˆ˜"] = 0.8 * df["temp_simple"]
    df["ì‹œê°„ëŒ€ê°€ì¤‘ì¹˜ì ìˆ˜"] = 0.8 * df["match_score"]

    df["ìµœì¢… ì†ì„±ê°€ì¤‘ì¹˜ì ìˆ˜"] = 0.8 * df["temp_simple"] + rating_weight * df["rating_bayes"]
    df["score_attrs"] = 0.8 * df["match_score"] + rating_weight * df["rating_bayes"]


    df["rank_simple"] = df["ìµœì¢… ì†ì„±ê°€ì¤‘ì¹˜ì ìˆ˜"].rank(ascending=False, method='min').astype(int)
    df["rank_final"] = df["score_attrs"].rank(ascending=False, method='min').astype(int)
    def make_rank_change_str(row):
        old_rank = row["rank_simple"]
        new_rank = row["rank_final"]
        diff = old_rank - new_rank
        if diff > 0:
            return f"{old_rank}ìœ„ â¡ {new_rank}ìœ„ (ğŸ”º{diff})"
        elif diff < 0:
            return f"{old_rank}ìœ„ â¡ {new_rank}ìœ„ (ğŸ”»{abs(diff)})"
        else:
            return f"{new_rank}ìœ„ (-)"

    df["rank_change"] = df.apply(make_rank_change_str, axis=1)

    print("[ë¹„êµ] ì†ì„± ê°€ì¤‘ì¹˜ ë¡œì§ vs ì†ì„±+ì‹œê°„ ê°€ì¤‘ì¹˜ ë¡œì§ ìˆœìœ„ ë³€ë™ í™•ì¸")
    cols = ["name","menu_title", "ì†ì„±ê°€ì¤‘ì¹˜ì ìˆ˜", "ì‹œê°„ëŒ€ê°€ì¤‘ì¹˜ì ìˆ˜", "ìµœì¢… ì†ì„±ê°€ì¤‘ì¹˜ì ìˆ˜", "score_attrs", "rank_change","ë² ì´ì§€ì•ˆ"]

    print("\n--- ì†ì„± ê°€ì¤‘ì¹˜ ì ìˆ˜(w=1) ê¸°ì¤€ Top 3 ---")
    print(df.sort_values("ìµœì¢… ì†ì„±ê°€ì¤‘ì¹˜ì ìˆ˜", ascending=False)[cols].head(5))

    print("\n--- ì†ì„± + ì‹œê°„ ê°€ì¤‘ì¹˜ ì ìˆ˜ ê¸°ì¤€ Top 3 ---")
    print(df.sort_values("score_attrs", ascending=False)[cols].head(5))

    display_df = df.sort_values("score_attrs", ascending=False)[cols].head(5)
    display(display_df)
    # print(df["match_score"].dtype, df["rating_bayes"].dtype)
    # print(df[["match_score", "rating_bayes"]].head())


    temp_df = df.copy()
    merged_meta = merged_df.copy()
    exclude_cols = ["menu_dict"]
    merge_target = merged_meta.drop(columns=exclude_cols)


    result = temp_df.merge(
        merge_target,
        on="restaurant_id",
        how="left"
    )
    result = result.sort_values(
        by=["score_attrs", "taste_score"],
        ascending=[False, False])

    result = result.drop_duplicates(
        subset=["restaurant_id", "menu_title"]
    ).reset_index(drop=True)

    # print("score_menus")
    display(result.head(2))
    return result.head(2)

import unicodedata
import ast

def score_menu_title(df, rating_weight=0.2):
    df = df.copy()

    def convert_to_list(x):
        if isinstance(x, str):
            try:
                return ast.literal_eval(x)
            except:
                return []
        return x

    df["title_tokens"] = df["title_tokens"].apply(convert_to_list)

    def clean_text(text):
        if not isinstance(text, str):
            return str(text)
        return unicodedata.normalize('NFC', text).replace(" ", "")

    menu_keywords = []
    normalized_search_tokens = [clean_text(t) for t in INTENT_STATE["tokens"]]

    for token in normalized_search_tokens:
        if df["title_tokens"].apply(lambda toks: token in clean_text("".join(toks) if isinstance(toks, list) else str(toks))).any():
            menu_keywords.append(token)

    menu_keywords = list(set(menu_keywords))
    print(f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {menu_keywords}")

    if not menu_keywords:
        df["score_title"] = 0
        return df


    def calculate_score(title_tokens):
        if not isinstance(title_tokens, list) or not title_tokens:
            return 0.0

        score = 0.0

        clean_tokens = [clean_text(t) for t in title_tokens]
        full_name = "".join(clean_tokens)

        for kw in menu_keywords:
            kw_clean = kw

            if kw_clean in clean_tokens:
                score += 1.0

            elif kw_clean in full_name:
                score += 0.7

        return score

    df["match_score"] = df["title_tokens"].apply(calculate_score)

    if menu_keywords:
        df["score_title"] = df["match_score"] / len(menu_keywords)
    else:
        df["score_title"] = 0


    df["ë² ì´ì§€ì•ˆ"] = rating_weight * df["rating_bayes"]
    # df["score_title"] = df["title_tokens"].apply(score)


    df["í‚¤ì›Œë“œë§¤ì¹­ì ìˆ˜"] = 0.8 * df["score_title"]
    df["score_title"] = 0.8 * df["score_title"] + rating_weight * df["rating_bayes"]

    temp_cols = ["name", "menu_title","match_score","score_title","title_tokens","ë² ì´ì§€ì•ˆ"]

    print("\n--- ë©”ë‰´ìŠ¤íŠ¸ë§ë§¤ì¹­ Top 3 ---")
    display(df.sort_values("score_title", ascending=False)[temp_cols].head(10))



    temp_df = df.copy()
    merged_meta = merged_df.copy()
    exclude_cols = ["menu_dict"]
    merge_target = merged_meta.drop(columns=exclude_cols)


    result = temp_df.merge(
        merge_target,
        on="restaurant_id",
        how="left"
    )

    result = result.sort_values(
        by=["score_title", "taste_score"],
        ascending=[False, False]
    )

    result = result.drop_duplicates(
        subset=["restaurant_id", "menu_title"]
    ).reset_index(drop=True)

    # print("score_menu_title")
    display(result)
    return result.head(2)

def recommend_menu(query, w_attr=0.7, w_title=0.3):
    tokens = preprocess_query(query)

    base_df = menu_store_df.copy()

    if INTENT_STATE["stations"]:
        st = INTENT_STATE["stations"]
        print(f"menu data station filter : {st}")
        print(f"menu data station filter ì ìš© ì „ : {len(base_df)} rows")
        display(base_df.head(5))
        base_df = base_df[base_df["station"].astype(str).isin(st)]
        print(f"menu data station filter ì ìš© í›„: {st} â†’ {len(base_df)} rows")
        display(base_df.head(5))

    df_attrs = score_menus(
        preprocess_dataframe(base_df.copy(), query),
    )
    # print(f"menus : {len(df_attrs)}")
    # display(f"menus : {df_attrs.head(2)}")
    # print(df_attrs.columns)

    df_title = score_menu_title(base_df.copy())
    # print(f"menus2 : {len(df_title)}")
    # display(f"menus2 : {df_title.head(2)}")
    # print(df_title.columns)

    return df_attrs,df_title

# def built_context(retrieve_df,
#     topk_df,
#     # mmr_df,
#     select_df_menu,
#     select_df_menu_title,):

#   context = {
#     "retrieve_review": retrieve_result_to_json(retrieve_df, topk_df),
#     # "mmr": convert_df_to_json(
#     #     mmr_df,
#     #     ["review_no","restaurant_id","restaurant_name","date","similarity"],
#     #     max_rows=10
#     # ),
#     "menus": convert_df_to_json(
#         select_df_menu,
#         ["id","name" ,"menu_title"],
#         max_rows=5
#     ),
#     "menus2": convert_df_to_json(
#         select_df_menu_title  ,
#         ["id","name" ,"menu_title"],
#         max_rows=5
#     )
#     }

#     print(f"context : {context}")

#     return context

"""## Chain

### í”„ë¡¬í”„íŠ¸
"""

conversation_history = []
last_llm_context = None

SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì„±ìˆ˜, ì„œìš¸ìˆ², ìì–‘, ê±´ëŒ€ì…êµ¬ ì§€ì—­ì˜ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
ë‹µë³€í•˜ê¸° ì „ì— ì•„ë˜ ì¡°ê±´ë“¤ì„ ë‹¨ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ìµœì ì˜ ê²°ë¡ ì„ ë„ì¶œí•˜ì„¸ìš”.
ê·¸ëŸ¬ë‚˜ ë¶„ì„ ê³¼ì •ì€ ì‚¬ìš©ìì—ê²Œ ê³µê°œí•˜ì§€ ë§ê³ ,
ìµœì¢… ê²°ë¡ ë§Œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì œì‹œí•˜ì„¸ìš”.

**ë°˜ë“œì‹œ ì œê³µëœ JSON ë°ì´í„°(context) ì™€ ì¶”ê°€ë¡œ ì œê³µë  ìˆ˜ ìˆëŠ” (parking_info) ì•ˆì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.**
JSONì— ì—†ëŠ” ì‹ë‹¹ / ë©”ë‰´ / í‰ê°€ / íŠ¹ì§•ì„ ìƒì„±í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
ì •ë³´ê°€ ë¶€ì¡±í•  ê²½ìš° ë‹¤ìŒ ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤:
"ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"

## context:
{context}

## ì‚¬ìš©ì ì§ˆë¬¸:
{question}


# â˜… ë‚´ë¶€ ì¶”ë¡  í”„ë¡œì„¸ìŠ¤ (ìµœì¢… ë‹µë³€ì— í¬í•¨ ê¸ˆì§€)

ë‹µë³€ì„ ìƒì„±í•˜ê¸° ì „ì—, ë°˜ë“œì‹œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì‹­ì‹œì˜¤:

1.  **ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ (Classification):** {question}ê³¼ chat_historyë¥¼ ê²€í† í•˜ì—¬ ì´ ì§ˆë¬¸ì´ "ì²˜ìŒ ì§ˆë¬¸"ì¸ì§€ "í›„ì† ì§ˆë¬¸"ì¸ì§€ íŒë³„í•©ë‹ˆë‹¤.
  1.1. chat_historyê°€ ë¹„ì–´ìˆìœ¼ë©´ â†’ â€œì²˜ìŒ ì§ˆë¬¸â€
  1.2. chat_historyê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ â†’ â€œí›„ì† ì§ˆë¬¸â€

2.  **ë°ì´í„° ì‚¬ìš© ê³„íš (Planning):**
    * "ì²˜ìŒ ì§ˆë¬¸": contextì˜ JSON ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ **ì¶”ì²œ ë¡œì§**ì„ ì‹¤í–‰í•˜ê³ , ì•„ë˜ ì§€ì •ëœ **í…ìŠ¤íŠ¸ ì¶œë ¥ í˜•ì‹**ì— ë”°ë¼ ì¶”ì²œ ëª©ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤.
    * "í›„ì† ì§ˆë¬¸": contextì˜ JSON ë°ì´í„° ë‚´ì—ì„œ {question}ì— ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë§Œ ê²€ìƒ‰í•˜ì—¬ **ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”** í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•  ê³„íšì„ ì„¸ì›ë‹ˆë‹¤.

3. "ì²˜ìŒ ì§ˆë¬¸" ê·œì¹™
  3.1. ì¶”ì²œ ë¡œì§:
  - ë¦¬ë·° ê¸°ë°˜: similarityë¥¼ ì´ìš©í•œ ê²°ê³¼ ê¸°ë°˜
  - ë©”ë‰´ ê¸°ë°˜: menus, menus2 ê¸°ë°˜
  - ê°€ê²©, ë¶„ìœ„ê¸°, ê±°ë¦¬ ë“± JSONì— ì—†ëŠ” ì •ë³´ ì–¸ê¸‰ ê¸ˆì§€
  - "ì„œë¡œë‹¤ë¥¸ ì‹ë‹¹" ëª¨ë‘ ì¶”ì²œ (ì—†ìœ¼ë©´ "ì—†ìŒ" ëª…ì‹œ)

  3.2.í…ìŠ¤íŠ¸ ì¶œë ¥ í˜•ì‹

  ë¦¬ë·°ê¸°ë°˜ê³¼ ë©”ë‰´ê¸°ë°˜ ì¶”ì²œ ì‹ë‹¹ 1ê°œ ë‹¹ ë‹¤ìŒ í•­ëª©ì„ **ìˆœì„œëŒ€ë¡œ** ë‚˜ì—´í•˜ì—¬ ì¶”ì²œ ëª©ë¡ì„ êµ¬ì„±í•˜ë©° ****3ê°œ ì´ìƒì˜ ìµœëŒ€í•œ ë§ì€****ì‹ë‹¹ì„ ì¶”ì²œí•©ë‹ˆë‹¤.(ë‹µë³€ì— ì§ì ‘ì ìœ¼ë¡œ ì´ ë©˜íŠ¸ë¥¼ í•˜ëŠ”ê±´ ê¸ˆì§€)

  1) ì¶”ì²œ ì‹ë‹¹ëª…: (ì‹ë‹¹ ì´ë¦„)
     - ì¶”ì²œ ë©”ë‰´: (contextì— ìˆëŠ” ë©”ë‰´ëª…ë“¤ì„ ì‰¼í‘œë¡œ ë‚˜ì—´: ë©”ë‰´1, ë©”ë‰´2, ë©”ë‰´3...)
     - ë©”ë‰´ íŠ¹ì§•: (ë§¤ìš´/êµ­ë¬¼/ë°”ì‚­í•¨/ê°€ì„±ë¹„/ë¶„ìœ„ê¸° ë“±, menus/menus2 ì†ì„± í•„ë“œê°€ ìˆì„ ê²½ìš°ì—ë§Œ ìš”ì•½)
     - Best review: "(review í•­ëª©ì´ ìˆì„ ê²½ìš° similarityê°€ ê°€ì¥ ë†’ì€ "text" ê°’ì„ ì›ë¬¸ ê·¸ëŒ€ë¡œâ€œ<...>â€ í˜•ì‹ìœ¼ë¡œ ì–¸ê¸‰)
     - ê°€ëŠ” ë°©ë²•: (distance_m, distance_from_station ë“± ê±°ë¦¬/ì‹œê°„ ì •ë³´ í™œìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì„œìˆ )
     - ì¶”ì²œ ì´ìœ : contextì— ìˆëŠ” ì¶”ì²œ ì‹ë‹¹ì˜ ëª¨ë“  ë¦¬ë·°ë¥¼ ì°¸ê³ í•˜ì—¬ ì¶”ì²œ ì‹ë‹¹ì˜ í•µì‹¬ íŠ¹ì§•, ì£¼ ë©”ë‰´ì˜ ì†ì„±, ì–¸ê¸‰ëœ ë¶„ìœ„ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±
     - URL : kakao_urlì— ìˆëŠ” ë§í¬ë¥¼ í´ë¦­í•˜ë©´ ì—°ê²°í•  ìˆ˜ ìˆë„ë¡ í•¨


  ìœ„ í•­ëª©ë“¤ ì¤‘, contextì— í•´ë‹¹ í•„ë“œê°€ ì—†ìœ¼ë©´ ê·¸ í•­ëª©ì€ ìƒëµ í•©ë‹ˆë‹¤.


4. "í›„ì† ì§ˆë¬¸" ê·œì¹™

4.1. ë‹µë³€ ê·œì¹™
  í›„ì† ì§ˆë¬¸ì€ ** ê¸°ì¡´ì— ì œê³µëœ {context}ì™€ ì¶”ê°€ë¡œ ì œê³µë  ìˆ˜ ìˆëŠ” parking_infoì™€ chat_history ê¸°ë°˜ ëŒ€í™”**ë¥¼ ëª©í‘œë¡œ í•˜ë©°, ë‹¤ìŒì„ ì ˆëŒ€ ê¸ˆì§€í•©ë‹ˆë‹¤:
  - ìƒˆë¡œìš´ ì‹ë‹¹, ë©”ë‰´, ë¦¬ë·° ì¶”ê°€ ìƒì„±
  - ì¶”ì²œ ë¡œì§ / í•„í„°ë§ / retrieval ì¬ìˆ˜í–‰ (ìƒˆë¡œìš´ ì¶”ì²œ ëª©ë¡ ìƒì„± ê¸ˆì§€)
  - ***ì²˜ìŒ ì§ˆë¬¸ì˜ í…ìŠ¤íŠ¸ ì¶œë ¥ í˜•ì‹ì„ ë”°ë¥´ì§€ ë§ê²ƒ***

4.2. í…ìŠ¤íŠ¸ êµ¬ì„± ê°€ì´ë“œ

  - ì‚¬ìš©ìê°€ "ì£¼ì°¨", "ì£¼ì°¨ì¥", "ì°¨ ê°€ì ¸ê°€ë„ ë¼?" ë“± ì£¼ì°¨ ê´€ë ¨ ì§ˆë¬¸ì„ í•˜ë©´:
    * FEWSHOT_PARKING ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤
    * ì—¬ëŸ¬ ì£¼ì°¨ì¥ì´ ìˆì„ ê²½ìš°, ê±°ë¦¬(distance)ê°€ ê°€ê¹ê±°ë‚˜ ë‚¨ì€ ìë¦¬(CPCTY - CUR_PRK_CNT)ê°€ ë” ë§ì€ ê³³ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¹„êµ ì„¤ëª…
  - ë‚ ì”¨/ì¸êµ¬ í˜¼ì¡ë„ ê´€ë ¨ í›„ì† ì§ˆë¬¸ì´ë©´:
    * weather_info, pop_status_info ì•ˆì— ìˆëŠ” ê°’ë§Œ ì‚¬ìš©í•´ì„œ í•œë‘ ë¬¸ì¥ìœ¼ë¡œë§Œ í˜„ì¬ ìƒí™©ì„ ì„¤ëª…í•œ í›„, í›„ì†ì§ˆë¬¸ì— ë§ëŠ” ë‹µë³€ìœ¼ë¡œ í™œìš©í•¨
  - ì´ì „ ëŒ€í™” ëª©ë¡ì¸ chat_historyë¥¼ ì¢…í•©í•˜ì—¬ ì§ˆë¬¸ ë‚´ìš©ì„ ì¶”ë¡ 

  - {context}ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ì¸¡/ìƒì„±í•˜ì§€ ë§ê³ ,
    ì •ë§ë¡œ ê´€ë ¨ í•„ë“œê°€ ì—†ìœ¼ë©´ "ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³ ë§Œ ë‹µë³€í•©ë‹ˆë‹¤.


5. ìµœì¢… ê²€ì¦ (Self-Verification)

5.1.  **ì¶œë ¥ í˜•ì‹ ê°•ì œ:** "ì²˜ìŒ ì§ˆë¬¸"ì— ëŒ€í•œ ë‹µë³€ì€ **ìœ„ì—ì„œ ì •ì˜ëœ í…ìŠ¤íŠ¸ ì¶”ì²œ ëª©ë¡ í˜•ì‹**ìœ¼ë¡œë§Œ êµ¬ì„±ë˜ì–´ì•¼ í•˜ë©°, **ì¶”ì²œ ì‹ë‹¹ëª…**ë¶€í„° ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.
5.2. ë¶€ì •ì ì¸ ë¦¬ë·°ê°€ ìˆëŠ” ê²½ìš°ì—ëŠ” ì¶”ì²œ ì‹ë‹¹ì—ì„œ ì œì™¸

----------------------------------------------------

# â˜… ì¤‘ìš” ìš”ì•½ (Agent Goal)

- ì²˜ìŒ ì§ˆë¬¸ â†’ **ì¶”ì²œ ì‹ë‹¹ ëª©ë¡** + ë¦¬ë·°/ë©”ë‰´/ì—­/ê±°ë¦¬ ë“±ì„ ìµœëŒ€í•œ í™œìš©í•´ì„œ ìµœëŒ€í•œ ë§ì€ ì‹ë‹¹ì„ ì œì•ˆ (ë‹¨, ëª¨ë‘ context ì•ˆì— ìˆëŠ” ê°’ë§Œ ì‚¬ìš©)
- í›„ì† ì§ˆë¬¸ â†’ chat_historyì™€ contextë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§¥ë½ ìˆê²Œ ë³´ì™„ ì„¤ëª…, **ìƒˆë¡œìš´ í›„ë³´ ìƒì„± ê¸ˆì§€**, ì£¼ì°¨/ë‚ ì”¨/í˜¼ì¡ë„ëŠ” parking_info, weather_info, pop_status_info í•„ë“œë§Œ í™œìš©
"""

FEWSHOT_PARKING = """
[Few-shot Example #1 â€” "Parking info" ì œê³µë˜ëŠ” ê²½ìš° ê²½ìš°]

System Context:
{{
  "parking_info": [
   {{
      "PRK_NM": "ì„±ìˆ˜1ê³µì˜ì£¼ì°¨ì¥",
      "CPCTY": "98",
      "CUR_PRK_CNT": "33",
      "distance": 121.4,
      "PAY_YN": "Y",
      "RATES": "1000",
      "TIME_RATES": "60",
      "ADD_RATES": "500",
      "ADD_TIME_RATES": "30"
      }}
  ],
  "retrieve_review": {{
    "restaurants": [{{ "restaurant_name": "ì„±ìˆ˜ìš°ë™" }}]
  }}
}}

Human:
ì„±ìˆ˜ìš°ë™ ê·¼ì²˜ì— ì£¼ì°¨í•  ê³³ ìˆì–´?

Assistant:
ë„¤! ì„±ìˆ˜ìš°ë™ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ì£¼ì°¨ì¥ì€ **ì„±ìˆ˜1ê³µì˜ì£¼ì°¨ì¥**ì…ë‹ˆë‹¤.
- ì´ 98ëŒ€ ì¤‘ 33ëŒ€ ì£¼ì°¨ â†’ 65ëŒ€ ë‚¨ìŒ
- ê±°ë¦¬: ì•½ 121m
- ìš”ê¸ˆ: 1ì‹œê°„ 1,000ì›, 30ë¶„ 500ì› ì¶”ê°€
ì‹¤ì‹œê°„ ì •ë³´ ì œê³µë©ë‹ˆë‹¤.


[Few-shot Example #2 â€” "parking info" ì—†ëŠ” ê²½ìš°]

System Context:
{{
  "parking_info": []
}}

Human:
ì„±ìˆ˜ìš°ë™ ê·¼ì²˜ ì£¼ì°¨ì¥ ì•Œë ¤ì¤˜

Assistant:
í˜„ì¬ ì œê³µëœ XML ë°ì´í„°ì—ëŠ” ì£¼ì°¨ì¥ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.
í•„ìš” ì‹œ ì£¼ì°¨ ì•±ì—ì„œ ì‹¤ì‹œê°„ í™•ì¸ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.


[Few-shot Example #3 â€” ì—¬ëŸ¬ê°œ ì£¼ì°¨ì¥ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°]

System Context:
{{
  "parking_info": [
    {{ "PRK_NM": "ì„±ìˆ˜1ê³µì˜ì£¼ì°¨ì¥", "CPCTY": "98", "CUR_PRK_CNT": "90", "distance": 200 }},
    {{ "PRK_NM": "ì„±ìˆ˜2ê³µì˜ì£¼ì°¨ì¥", "CPCTY": "120", "CUR_PRK_CNT": "50", "distance": 350 }}
  ]
}}

Human:
ì„±ìˆ˜ ê·¼ì²˜ ì£¼ì°¨ ì–´ë””ê°€ ì¢‹ì•„?

Assistant:
ê°€ì¥ ê°€ê¹Œìš´ ê³³ì€ **ì„±ìˆ˜1ê³µì˜ì£¼ì°¨ì¥(200m)** ì´ì§€ë§Œ ë‚¨ì€ ê³µê°„ì€ 8ëŒ€ì…ë‹ˆë‹¤.
ì—¬ìœ  ìˆëŠ” ì£¼ì°¨ë¥¼ ì›í•œë‹¤ë©´ **ì„±ìˆ˜2ê³µì˜ì£¼ì°¨ì¥(ë‚¨ì€ ìë¦¬ 70ëŒ€)** ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.
"""


HUMAN_PROMPT = "{question}"

"""### pipe line"""

prompt = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT),
    ("system", FEWSHOT_PARKING),
    MessagesPlaceholder("chat_history"),
    ("system", "ë‹¤ìŒ JSONì„ ì°¸ê³ í•˜ì„¸ìš”:\n{context}"),
    ("human", HUMAN_PROMPT)
])


llm = ChatOpenAI(model="gpt-4o-mini")

rag_chain = prompt | llm


def fix_timestamps(obj):
    if isinstance(obj, pd.Timestamp):
        return obj.isoformat()

    if isinstance(obj, dict):
        return {k: fix_timestamps(v) for k, v in obj.items()}

    if isinstance(obj, (list, tuple)):
        container = type(obj)
        return container(fix_timestamps(v) for v in obj)
    return obj


def convert_df_to_json(df, columns, max_rows=20):
    df = df[columns].head(max_rows)
    return df.to_dict(orient="records")

def format_docs_for_prompt(docs):
    formatted = []
    for d in docs:
        if isinstance(d.page_content, dict):
            formatted.append(d.page_content)
        else:
            formatted.append({"content": d.page_content})
    return json.dumps(formatted, ensure_ascii=False, indent=2)

def invoke_rag_chain(
    user_query,
    llm_context,
    conversation_history
):

    llm_response = rag_chain.invoke({
        "question": user_query,
        "context": llm_context,
         "chat_history": conversation_history
    })


    conversation_history.append(HumanMessage(content=user_query))
    conversation_history.append(AIMessage(content=llm_response.content))


    return llm_response.content, conversation_history

"""## ì§ˆë¬¸ì£¼ì œêµ¬ë¶„"""

def is_parking_query(text):
    parking_keywords = [
        "ì£¼ì°¨", "ì£¼ì°¨ì¥", "ë°œë ›", "ì£¼ì°¨ë¹„", "ê³µì˜ì£¼ì°¨ì¥",
        "ì°¨ ëŒ€", "íŒŒí‚¹", "ì£¼ì°¨ ê°€ëŠ¥", "ë§Œì°¨", "ì •ì‚°"
    ]
    return any(kw in text for kw in parking_keywords)

def is_weather_query(text):
    weather_keywords = [
        "ë‚ ì”¨", "ê¸°ì˜¨", "ì˜¨ë„", "ì¶”ì›Œ", "ë”ì›Œ", "ë”ìš´", "ì¶”ìš´", "ì‹œì›í•œ",
        "ë”°ëœ»í•œ", "ë¹„ì™€", "ëˆˆì™€", "ë§‘ì•„", "íë ¤", "ê°•ìˆ˜", "ë°”ëŒ", "ìŠµë„",
        "ë¯¸ì„¸ë¨¼ì§€", "ë¨¼ì§€", "ë°”ê¹¥", "ì‹¤ë‚´", "ë°–ì—", "ì•¼ì™¸", "ë£¨í”„íƒ‘", "í…Œë¼ìŠ¤"
    ]
    return any(kw in text for kw in weather_keywords)

INTENT_STATE = {
  "tokens": [],
  "stations": [],
  "time_slot": None
}

"""## ì±—ë´‡

### ììœ¨í˜• ì±—ë´‡
"""

def get_matched_restaurants(llm_context, restaurant_name_tokens):
    all_restaurants_list = []
    for restaurant_data in llm_context.get("retrieve_review", {}).get("restaurants", []):
        all_restaurants_list.append({
            "restaurant_id": restaurant_data.get("restaurant_id"),
            "restaurant_name": restaurant_data.get("restaurant_name"),
            "meta": restaurant_data.get("meta", {})
        })

    for menu_data in llm_context.get("menus", []):
        all_restaurants_list.append({
            "restaurant_id": menu_data.get("restaurant_id"),
            "restaurant_name": menu_data.get("restaurant_name_db"),
            "meta": restaurant_data.get("meta", {})
        })

    for menu_title_data in llm_context.get("menus2", []):
        all_restaurants_list.append({
            "restaurant_id": menu_title_data.get("restaurant_id"),
            "restaurant_name": menu_title_data.get("restaurant_name_db"),
            "meta": restaurant_data.get("meta", {})
        })

    unique_restaurants_dict = {
        restaurant_info["restaurant_id"]: restaurant_info
        for restaurant_info in all_restaurants_list if restaurant_info.get("restaurant_id") is not None
    }

    matched_restaurants_list = [
        restaurant_info for restaurant_info in list(unique_restaurants_dict.values())
        if restaurant_info.get("restaurant_name") and any(token in restaurant_info["restaurant_name"] for token in restaurant_name_tokens)
    ]

    return matched_restaurants_list

def standard_run_query(query, history):
  try:
    import gradio as gr
    global conversation_history, last_llm_context, INTENT_STATE, city_datas
    llm_context = {}
    print(f"query: {query}")

    intent = extract_query_intent(query)

    if intent["tokens"]:
        INTENT_STATE["tokens"] = intent["tokens"]
    if intent["stations"]:
        INTENT_STATE["stations"] = intent["stations"]
    if intent["time_slot"]:
        INTENT_STATE["time_slot"] = intent["time_slot"]

    print("INTENT_STATE:", INTENT_STATE)



    pop_status_df = pd.DataFrame()
    weather_df = pd.DataFrame()
    rag_retrieve_df = pd.DataFrame()
    rag_time_weight_df = pd.DataFrame()
    rag_mmr_df = pd.DataFrame()
    select_df_menu = pd.DataFrame()
    select_df_menu_title = pd.DataFrame()

    is_first_question = (last_llm_context is None and len(conversation_history) == 0)
    print(f"is_first_question = {is_first_question}")

    if is_first_question:
        city_datas = get_city_datas()
        print(city_datas)
        station = INTENT_STATE["stations"][0] if INTENT_STATE["stations"] else None
        if( station is not None ):
          lo = get_station_to_location(station)
          pop_status_df = get_pop_status_data(city_datas.get(lo))
          display(pop_status_df)
          weather_df = get_weather_data(city_datas.get(lo))
          display(weather_df)

        rag_retrieve_df = rag_recommend_reviews(query)
        temp_rag_retrieve_df, rag_time_weight_df = rag_recommend_reviews_with_time_weight(query)
        rag_mmr_df = select_reviews_mmr(query)
        select_df_menu, select_df_menu_title = recommend_menu(query)

        llm_context = {
            "pop_status_info": pop_status_df.to_dict(orient="records"),
            "weather_info": weather_df.to_dict(orient="records"),
            "retrieve_review": retrieve_result_to_json(temp_rag_retrieve_df, rag_time_weight_df),
            "menus": select_df_menu.to_dict(orient="records"),
            "menus2": select_df_menu_title.to_dict(orient="records")
        }

        print(f"llm_context : {llm_context}")
        llm_answer, conversation_history = invoke_rag_chain(query, llm_context, conversation_history)
        last_llm_context = llm_context
        history.append((query, llm_answer))
        print(f"llm_answer : {llm_answer}")
        return history, "", pop_status_df, weather_df, rag_retrieve_df, rag_time_weight_df, rag_mmr_df, select_df_menu, select_df_menu_title

    else:
        if is_parking_query(query):
          restaurant_name_tokens = INTENT_STATE["tokens"]
          print("restaurant_name_tokens:", restaurant_name_tokens)
          matched_restaurants = get_matched_restaurants(last_llm_context, restaurant_name_tokens)
          print("matched_restaurants:", matched_restaurants)

          if matched_restaurants:
              rid = matched_restaurants[0]["restaurant_id"]
              print("rid:", rid)
              lo = get_station_to_location(INTENT_STATE["stations"][0])
              print("lo:", lo)

              # temp_df = pd.DataFrame([matched_restaurants[0]["meta"]])
              # print("temp_df:", temp_df)
              print(f"is_parking_query city_datas : {city_datas}")
              parking_info = get_parking_info(
                  pd.DataFrame([matched_restaurants[0]["meta"]]), city_datas.get(lo)
              ).head(3)
              print("Parking Info:", parking_info)
          else:
              if INTENT_STATE["stations"]:
                        lo = get_station_to_location(INTENT_STATE["stations"][0])
                        parking_info = get_parking_info_by_location(
                            lo,
                            city_datas.get(lo)
                        ).head(3)
              else:
                  parking_info = pd.DataFrame()

          last_llm_context['parking_Info'] = parking_info.to_dict(orient="records")
          display(parking_info)

    print("last_llm_context:", last_llm_context)
    llm_answer, conversation_history = invoke_rag_chain(query, last_llm_context, conversation_history)
    history.append((query, llm_answer))
    print(f"llm_answer : {llm_answer}")
    return history, "", gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), gr.update()

  except Exception as e:
    print(f"Error: {type(e).__name__}: {e}")
    return history, "", gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), gr.update(), gr.update()


def reset_conversation():
    global conversation_history, last_llm_context, INTENT_STATE
    print("DEBUG: RESET CALLED")
    conversation_history = []
    last_llm_context = None
    INTENT_STATE = {
        "tokens": [],
        "stations": [],
        "time_slot": None
    }
    city_datas = {}
    return [], ""

"""### ê°€ì´ë“œí˜• ì±—ë´‡"""

# def step1_selected(evt: gr.SelectData):
#     # evt.valueëŠ” ì„ íƒëœ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
#     return gr.update(visible=True), f"ì„ íƒëœ ìœ„ì¹˜: {evt.value}"

# def step2_selected(evt: gr.SelectData):
#     return gr.update(visible=True), f"ì„ íƒëœ ëª©ì : {evt.value}"

# def step3_selected(evt: gr.SelectData):
#     return gr.update(visible=True), f"ì„ íƒëœ ë©”ë‰´: {evt.value}"

# def reset_all():
#     return (
#         gr.update(value=None),
#         gr.update(value=None, visible=False),
#         gr.update(value=None, visible=False),
#         gr.update(visible=False),
#         []
#     )

def custom_run_query(query, history):
  return

"""## Gradio

### UI
"""

import gradio as gr

with gr.Blocks(theme="soft", css="body {zoom: 1.3;}") as demo:
    gr.Markdown("### ğŸ¥š ë©”ì¶”ë¦¬ì•Œ!! ì„±ìˆ˜/ì„œìš¸ìˆ²/ê±´ëŒ€ ë§›ì§‘ ì¶”ì²œ ì±—ë´‡")

    with gr.Row():
        mode_selector = gr.Radio(
            ["ììœ¨ ê²€ìƒ‰í˜• (í‚¤ì›Œë“œ)", "ê°€ì´ë“œë¼ì¸í˜• (ì‹œë‚˜ë¦¬ì˜¤)"],
            label="ì‚¬ìš© ëª¨ë“œ ì„ íƒ",
            value="ììœ¨ ê²€ìƒ‰í˜• (í‚¤ì›Œë“œ)"
        )

    with gr.Row():
        chatbot = gr.Chatbot(height=500)

    with gr.Row():
        msg = gr.Textbox(label="ì§ˆë¬¸ ì…ë ¥", scale=4)
        submit = gr.Button("ì „ì†¡", scale=1)
        reset_btn = gr.Button("ìƒˆ ëŒ€í™” ì‹œì‘", scale=1)

    with gr.Accordion("ë¶„ì„ ë°ì´í„°", open=True):
        with gr.Tabs():
            with gr.TabItem("ì‹¤ì‹œê°„ ë°ì´í„°"):
                with gr.Row():
                    pop_status_df = gr.Dataframe(label="ì‹¤ì‹œê°„ ì¸êµ¬ë°ì´í„°")
                    weather_df = gr.Dataframe(label="ì‹¤ì‹œê°„ ë‚ ì”¨ë°ì´í„°")

            with gr.TabItem("RAG ì¶”ì²œ"):
                with gr.Row():
                    rag_retrieve_df = gr.Dataframe(label="1ì°¨: ë¦¬ë·° ê¸°ë°˜ ì¶”ì²œ (Retrieve)")
                    rag_time_weight_df = gr.Dataframe(label="2ì°¨: ì‹œì˜ì„± ê°€ì¤‘ì¹˜ (Time Weight)")
                    rag_mmr_df = gr.Dataframe(label="3ì°¨: ë‹¤ì–‘ì„± í™•ë³´ (MMR)")

            with gr.TabItem("ë©”ë‰´ ì¶”ì²œ"):
                with gr.Row():
                    menu_attr_out = gr.Dataframe(label="ì¶”ì¶œëœ ë©”ë‰´ ì†ì„±", col_count=(3, "fixed"))
                    menu_title_out = gr.Dataframe(label="ë©”ë‰´ íƒ€ì´í‹€", col_count=(3, "fixed"))

    submit.click(
        fn=standard_run_query,
        inputs=[msg, chatbot],
        outputs=[chatbot, msg, pop_status_df, weather_df, rag_retrieve_df, rag_time_weight_df, rag_mmr_df, menu_attr_out, menu_title_out]
    )

    reset_btn.click(
        fn=reset_conversation,
        inputs=None,
        outputs=[chatbot, msg]
    )

demo.launch(share=True, debug=True, show_error=True)

"""## ì‚¬ìš©ì ì…ë ¥




"""

# def run_query(_):
#     global chat_history

#     query = text_box.value.strip()
#     if query == "0":
#         print("END")
#         return chat_history

#     print(f"ì‚¬ìš©ì ì¿¼ë¦¬ : {query}\n")

#     retrieve_df, topk_df = recommend_reviews(query)

#     # mmr_df = select_reviews_mmr(query, lambda_mult=0.5, top_k=5, verbose=False)

#     select_df_menu, select_df_menu_title = recommend_menu(query)

#     answer = invoke_rag_chain(
#         retrieve_df,
#         topk_df,
#         # mmr_df,
#         select_df_menu,
#         select_df_menu_title,
#         query,
#         chat_history
#     )

#     print(answer)

#     return chat_history

# text_box = widgets.Text(
#     placeholder="ê²€ìƒ‰ì–´ (0 ì…ë ¥ ì‹œ ì¢…ë£Œ)",
#     description="ê²€ìƒ‰ì–´ : ",
#     style={'description_width': 'initial'},
# )
# run_button = widgets.Button(description="BEGIN")

# run_button.on_click(run_query)

# display(text_box, run_button)

