# -*- coding: utf-8 -*-
"""Menu_Select.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y_MOj3nmD3-CsQDzi6zGOooscPjQ_Fsr
"""

# from google.colab import drive
# drive.mount('/content/drive')

# import pandas as pd

menu_df = pd.read_csv('/content/drive/MyDrive/CapStone/캡스톤/3.menu_list_total.csv')
menu_df.rename(columns={"id": "restaurant_id"}, inplace=True)
menu_df["menu_id"] = (
    (menu_df.groupby("restaurant_id").cumcount() + 1).astype(str)
)

menu_df.head()

restaurant_df = pd.read_csv('/content/drive/MyDrive/CapStone/캡스톤/1.id_v2.csv')
restaurant_df.rename(columns={"id": "restaurant_id"}, inplace=True)
restaurant_df.head()

review_sentiment_df = pd.read_excel('/content/drive/MyDrive/CapStone/캡스톤/5. review_sentence V7.xlsx')
review_sentiment_df.rename(columns={"id": "restaurant_id"}, inplace=True)
review_sentiment_df.head()

for dataframe in [menu_df, restaurant_df, review_sentiment_df]:
    if "id" in dataframe.columns:
        dataframe["id"] = dataframe["id"].astype(str).str.replace(r"\.0$", "", regex=True)
    if "restaurant_id" in dataframe.columns:
        dataframe["restaurant_id"] = dataframe["restaurant_id"].astype(str).str.replace(r"\.0$", "", regex=True)
    if "menu_id" in dataframe.columns:
        dataframe["menu_id"] = dataframe["menu_id"].astype(str).str.replace(r"\.0$", "", regex=True)
    if "review_no" in dataframe.columns:
        dataframe["review_no"] = dataframe["review_no"].astype(str).str.replace(r"\.0$", "", regex=True)

print(menu_df.head())
print(restaurant_df.head())
print(review_sentiment_df.head())

review_sentiment_df['rating'].isna().sum(), review_sentiment_df['rating'].describe()

restaurant_mean_rating = review_sentiment_df.groupby("restaurant_id")["rating"].transform("mean")
review_sentiment_df["rating"] = review_sentiment_df["rating"].fillna(restaurant_mean_rating)
review_sentiment_df["rating"] = review_sentiment_df["rating"].fillna(review_sentiment_df["rating"].mean())

restaurant_score_df = review_sentiment_df.groupby(["restaurant_id", "name"], as_index=False).agg(
    rating_mean=("rating", "mean"),
    review_count=("review_no", "count")
)

def bayesian_avg(rating, review_count, global_mean, min_reviews=30):
    return (review_count / (review_count + min_reviews)) * rating + (min_reviews / (review_count + min_reviews)) * global_mean


print(restaurant_score_df["review_count"].describe())

global_mean_rating = restaurant_score_df["rating_mean"].mean()


restaurant_score_df["rating_bayes"] = restaurant_score_df.apply(
    lambda row: bayesian_avg(row["rating_mean"], row["review_count"], global_mean_rating, min_reviews=30),
    axis=1
)

print(restaurant_score_df["review_count"].describe())


restaurant_score_df.head()

restaurant_score_df = restaurant_score_df.merge(
    restaurant_df[["restaurant_id", "name", "category_1"]],
    on="restaurant_id", how="left"menu_with_store = menu_df.merge(
    restaurant_score_df[["restaurant_id", "rating_bayes", "category_1"]],
    on="restaurant_id",
    how="left"
)

)
restaurant_score_df.rename(columns={"name_y": "name"}, inplace=True)
restaurant_score_df.drop(columns=["name_x"], errors="ignore", inplace=True)


restaurant_score_df.head()

menu_with_store = menu_df.merge(
    restaurant_score_df[["name", "rating_bayes", "category_1"]],
    on="name", how="left"
)

menu_with_store = menu_with_store.drop_duplicates(
    subset=["restaurant_id", "menu_id"]
)

menu_with_store["rating_bayes"] = menu_with_store["rating_bayes"].fillna(global_mean_rating)

menu_with_store.head()

menu_with_store.sort_values("restaurant_id", ascending=True)

menu_with_store.head()

menu_with_store.to_csv('/content/drive/MyDrive/Ehwa/캡스톤/menu_with_store.csv', index=False, encoding='utf-8-sig')



menu_with_store.head()

# !pip install konlpy
from konlpy.tag import Okt

okt_tokenizer = Okt()

def normalize_tokens(text, is_use_v=True):
    text = re.sub(r"[^가-힣a-zA-Z0-9\s]", " ", text)
    okt_pos_tags = okt_tokenizer.pos(text, stem=True)

    base_token_list = []
    for word, pos_tag in okt_pos_tags:
        if pos_tag in ["Noun", "Adjective", "Verb"]:
            base_token_list.append(word)

    if is_use_v:
        valid_prefixes = ("N", "V", "VA", "XR")  # 동사 포함
    else:
        valid_prefixes = ("N", "VA", "XR")       # 동사 제외

    token_set = set()
    for word, pos_tag in okt_pos_tags:
        # 품사 조건
        if pos_tag == "Noun":
            token_set.add(word)
        elif pos_tag == "Adjective":
            token_set.add(word)
        elif is_use_v and pos_tag == "Verb":  # 동사 포함 여부
            token_set.add(word)

    cleaned_tokens = set()
    for word in token_set:
        if word.endswith(("하다", "한")):
            base_word = re.sub(r"(하다|한)$", "", word)
            if len(base_word) >= 2:
                cleaned_tokens.add(base_word)
        cleaned_tokens.add(word)

    return list(cleaned_tokens)

#각 메뉴 타이틀, tokenize
import re
df = pd.read_csv('/content/drive/MyDrive/Ehwa/캡스톤/menu_with_store.csv')
menu_with_tokens_df  = df.copy()
menu_with_tokens_df.head()

menu_with_tokens_df["title_tokens"] = menu_with_tokens_df["menu_title"].astype(str).apply(normalize_tokens)
menu_with_tokens_df.head()

menu_with_tokens_df.to_csv('/content/drive/MyDrive/Ehwa/캡스톤/menu_with_token.csv', index=False, encoding='utf-8-sig')

# !pip install kiwipiepy

import pandas as pd
from collections import Counter, defaultdict
from math import log
from kiwipiepy import Kiwi
kiwi = Kiwi()

menu_data_raw_df = pd.read_csv('/content/drive/MyDrive/Ehwa/menu_with_store.csv')
menu_data_df  = menu_data_raw_df.copy()
menu_data_df.head()

# STOPWORDS = set(["추가","메뉴","추천","세트","정식","수제","신메뉴","뷔페"])

# import re

# def clean_menu_dataframe(df):
#     """
#     STOPWORDS 포함 메뉴명/설명 제거.
#     내부적으로만 적용되며 사용자 쿼리엔 영향 없음.
#     """
#     def has_stopword(text):
#         clean_text = re.sub(r"\s+", "", str(text))  # 공백 제거
#         for sw in STOPWORDS:
#             if re.search(rf"{re.escape(sw)}", clean_text):
#                 return True
#         return False

#     # menu_title 기준으로 stopword 포함된 행 제거
#     mask = ~df["menu_title"].apply(has_stopword)
#     cleaned = df[mask].copy()
#     return cleaned

# menu_data_df = clean_menu_dataframe(menu_data_df)

import re

# TEXT_COLS = ["menu_title", "desc"]
# TEXT_COLS = [c for c in ["menu_title", "desc", "category_1"] if c in menu_data_df.columns]
TEXT_COLS = ["menu_title"]

ATTRIBUTE_COLS = ["spicy", "vegan", "meat", "seafood",
                  "diet", "dessert", "healthy",
                  "bakery", "alcohol_pair", "beverage_alcohol",
                  "soup", "cold", "hot"]

ATTR_TRANSLATIONS = {
    "spicy": ["매운", "얼큰한", "칼칼한", "자극적인"],
    "vegan": ["비건", "채식", "채소만", "고기없는"],
    "meat": ["고기", "삼겹살", "소고기", "돼지고기", "닭고기", "육류"],
    "seafood": ["해산물", "생선", "새우", "조개", "오징어", "회"],
    "diet": ["다이어트", "저칼로리", "가볍게", "살안찌는"],
    "dessert": ["디저트", "케이크", "빙수", "아이스크림", "달달한"],
    "healthy": ["건강식", "영양식", "담백한", "저염식"],
    "bakery": ["베이커리", "빵", "크로와상", "바게트"],
    "alcohol_pair": ["안주", "술안주", "맥주안주", "소주안주"],
    "beverage_alcohol": ["음료", "술", "맥주", "소주", "칵테일", "커피"],
    "soup": ["국물", "찌개", "탕", "국"],
    "cold": ["차가운", "시원한", "냉면", "냉국수"],
    "hot": ["따뜻한", "뜨거운", "데운", "온"]
}


def normalize_attr_dict(attribute_dict):
    normalized_attribute_dict = {}
    for attribute_name, keyword_list in attribute_dict.items():
        normalized_keyword_set = set()
        for keyword in keyword_list:

            kiwi_tokens = kiwi.tokenize(keyword)
            for token in kiwi_tokens:
                if token.tag.startswith(("N", "VA", "XR")):
                    normalized_keyword_set.add(token.form)
                    if token.lemma != token.form:
                        normalized_keyword_set.add(token.lemma)

            if keyword.endswith("하다") or keyword.endswith("한"):
                base_keyword = re.sub(r"(하다|한)$", "", keyword)
                if len(base_keyword) >= 2:
                    normalized_keyword_set.add(base_keyword)
        normalized_attribute_dict[attribute_name] = list(normalized_keyword_set)
    return normalized_attribute_dict

ATTR_TRANSLATIONS_NORM = normalize_attr_dict(ATTR_TRANSLATIONS)

print(ATTR_TRANSLATIONS_NORM)

# df = pd.read_csv('/content/drive/MyDrive/Ehwa/캡스톤/menu_with_store.csv')
# menu_store_Df  = df.copy()
# menu_store_Df["title_tokens"] = menu_store_Df["menu_title"].astype(str).apply(normalize_tokens)
# menu_store_Df.head()

from kiwipiepy import Kiwi
kiwi = Kiwi()

def tokenize(text):
    token_list = []
    for token in kiwi.tokenize(text):
        if token.tag.startswith(("N", "V", "VA", "XR")):
            token_list.append(token.form)
            if token.lemma != token.form:
                token_list.append(token.lemma)
    return token_list


def detect_attributes(user_query, attribute_dict, threshold=0):
    query_token_list = tokenize(user_query)
    attribute_score_dict = {attribute_name: 0 for attribute_name in attribute_dict}

    for attribute_name, keyword_list in attribute_dict.items():
        for token in query_token_list:
            if token in keyword_list:
                attribute_score_dict[attribute_name] += 1

    detected_attribute_list = [attribute_name for attribute_name, score in attribute_score_dict.items() if score > threshold]
    detected_attribute_list = sorted(detected_attribute_list, key=lambda attr: attribute_score_dict[attr], reverse=True)

    return detected_attribute_list if detected_attribute_list else None

# query = "시원하고 얼큰한 국물 요리 추천해줘"
# print(tokenize(query))
# print(detect_attributes(query, ATTR_TRANSLATIONS_NORM))

def score_menus(menu_dataframe, detected_attribute_list, rating_weight=0.3):
    if not detected_attribute_list:
        return menu_dataframe.copy()

    scored_menu_df = menu_dataframe.drop_duplicates(subset=["id"]).copy()

    def calculate_match_score(menu_row):
        matched_attribute_count = sum(menu_row[attribute_name] for attribute_name in detected_attribute_list if attribute_name in menu_row)
        return matched_attribute_count / len(detected_attribute_list)
    scored_menu_df["match_score"] = scored_menu_df.apply(calculate_match_score, axis=1)

    scored_menu_df["final_score"] = (
        0.7 * scored_menu_df["match_score"] + rating_weight * scored_menu_df["rating_bayes"]
    )
    scored_menu_df = scored_menu_df.sort_values("final_score", ascending=False)
    return scored_menu_df

def preprocess_dataframe(menu_dataframe, user_query):
    filtered_df = menu_dataframe.copy()
    if "뷔페" not in user_query:
      filtered_df = filtered_df[~(
          (filtered_df["category_1"].str.contains("뷔페", na=False)) |
          (filtered_df["menu_title"].str.contains("뷔페", na=False))
      )]
    if "세트" not in user_query:
      filtered_df = filtered_df[~(
          (filtered_df["menu_title"].str.contains("정식", na=False))|
          (filtered_df["menu_title"].str.contains("세트", na=False))
      )]
    return filtered_df



from IPython.display import display
import ipywidgets as widgets

def run_query(button_event):
    user_query_text = text_box.value.strip()
    if user_query_text == "0":
        print(" END")
        return

    print(user_query_text)
    detected_attribute_list = detect_attributes(user_query_text, ATTR_TRANSLATIONS_NORM)
    print(f"속성: {detected_attribute_list}")
    if not detected_attribute_list:
        return

    preprocessed_menu_df = preprocess_dataframe(menu_data_df, user_query_text)

    scored_menu_result_df = score_menus(preprocessed_menu_df, detected_attribute_list)

    if scored_menu_result_df.empty:
        print("해당 속성에 맞는 메뉴를 찾지 못했습니다.")
    else:
        # print(scored_menu_result_df.head(5))
        display(scored_menu_result_df.head(5))

text_box = widgets.Text(
    placeholder="검색어 (0 입력 시 종료)",
    description="검색어 : ",
    style={'description_width': 'initial'},
)
run_button = widgets.Button(description="BEGIN")

run_button.on_click(run_query)
display(text_box, run_button)

