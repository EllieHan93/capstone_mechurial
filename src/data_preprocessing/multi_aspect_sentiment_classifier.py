# -*- coding: utf-8 -*-
"""Bert+MLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R-IkELZllFWZV5GiV9ryLkm353Q6I3Ry
"""

# !pip install -q transformers datasets accelerate scikit-learn

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

labeled_review_df = pd.read_csv("/content/drive/MyDrive/Ehwa/gpt_classified_reviews_divided.csv")

ASPECT_COLS = ["Í∞ÄÍ≤©", "ÏÑúÎπÑÏä§", "Îßõ", "Î∂ÑÏúÑÍ∏∞"]
TEXT_COL = "sentence"

labeled_review_df.head()

labeled_review_df.rename(columns={'Í∞ÄÍ≤©':'price', 'ÏÑúÎπÑÏä§': 'service', 'Îßõ' : 'taste', 'Î∂ÑÏúÑÍ∏∞' : 'embience'})

LABEL2ID = {"Î∂ÄÏ†ï":0, "Ï§ëÎ¶Ω":1, "Í∏çÏ†ï":2}
ID2LABEL = {v:k for k,v in LABEL2ID.items()}

import re, ast
def clean_label(label):
    if pd.isna(label): return "Ï§ëÎ¶Ω"
    if isinstance(label, list): return label[0]
    cleaned_label = str(label).strip()
    cleaned_label = re.sub(r"[\[\]'\" ]","", cleaned_label)
    if cleaned_label not in LABEL2ID: return "Ï§ëÎ¶Ω"
    return cleaned_label

for aspect_col in ASPECT_COLS:
    labeled_review_df[aspect_col] = labeled_review_df[aspect_col].map(clean_label)

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

MODEL_NAME = "klue/bert-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

class MultiAspectDataset(Dataset):
    def __init__(self, df, tokenizer, text_col, aspect_cols, label2id, max_len=128):
        self.df = df.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.text_col = text_col
        self.aspect_cols = aspect_cols
        self.label2id = label2id
        self.max_len = max_len

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        enc = self.tokenizer(
            row[self.text_col],
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )
        enc = {k: v.squeeze(0) for k, v in enc.items()}

        # ÏÜçÏÑ±Î≥Ñ ÎùºÎ≤® ÌÖêÏÑúÎ°ú
        labels = {aspect: torch.tensor(self.label2id[row[aspect]], dtype=torch.long)
                  for aspect in self.aspect_cols}
        return enc, labels

    def __len__(self):
        return len(self.df)

dataset = MultiAspectDataset(labeled_review_df, tokenizer, TEXT_COL, ASPECT_COLS, LABEL2ID)

import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel

class MLPHead(nn.Module):
    def __init__(self, in_dim=768, hidden_dim=256, num_labels=3, dropout_prob=0.2):
        super().__init__()
        self.dropout1 = nn.Dropout(dropout_prob)
        self.fc1   = nn.Linear(in_dim, hidden_dim)
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.dropout2 = nn.Dropout(dropout_prob)
        self.output_layer   = nn.Linear(hidden_dim, num_labels)
    def forward(self, x):
        x = self.dropout1(x)
        x = F.gelu(self.fc1(x))
        x = self.norm1(x)
        x = self.dropout2(x)
        return self.output_layer(x)

class MultiAspectClassifier(nn.Module):
    def __init__(self, model_name="klue/bert-base", aspects=None, num_labels=3):
        super().__init__()
        self.backbone = AutoModel.from_pretrained(model_name)
        hidden_size = self.backbone.config.hidden_size
        self.heads = nn.ModuleDict({
            aspect: MLPHead(hidden_size, 256, num_labels) for aspect in aspects
        })
    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):
        backbone_output = self.backbone(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        cls_token_embedding = backbone_output.last_hidden_state[:,0,:]  # [CLS]
        logits = {aspect: head(cls_token_embedding) for aspect, head in self.heads.items()}
        loss = None
        if labels:
            aspect_losses = []
            for aspect in self.heads:
                aspect_losses.append(F.cross_entropy(logits[aspect], labels[aspect]))
            loss = torch.stack(aspect_losses).mean()
        return logits, loss

from sklearn.model_selection import train_test_split

train_valid_df, test_df = train_test_split(
    labeled_review_df,
    test_size=0.1,                 # 10%Îßå test
    stratify=labeled_review_df["Îßõ"],           # ÌÅ¥ÎûòÏä§ ÎπÑÏú® Ïú†ÏßÄ
    random_state=42
)


print(f"Train+Valid: {len(train_valid_df)}, Test: {len(test_df)}")

from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)



from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
model = MultiAspectClassifier(model_name=MODEL_NAME, aspects=ASPECT_COLS).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

loader = DataLoader(dataset, batch_size=8, shuffle=True)

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
import numpy as np
import torch
from torch.utils.data import DataLoader

def evaluate_aspectwise(model, dataloader, aspects, id2label=None):
    model.eval()
    predicted_labels = {aspect: [] for aspect in aspects}
    true_labels = {aspect: [] for aspect in aspects}

    with torch.no_grad():
        for tokenized_input, labels in dataloader:
            tokenized_input = {key: value.to(device) for key, value in tokenized_input.items()}
            labels = {aspect: label.to(device) for aspect, label in labels.items()}
            logits, _ = model(**tokenized_input)
            for aspect in aspects:
                predicted = torch.argmax(logits[aspect], dim=-1).cpu().numpy()
                ground_truth = labels[aspect].cpu().numpy()
                predicted_labels[aspect].extend(predicted)
                true_labels[aspect].extend(ground_truth)

    aspect_metrics = {}
    for aspect in aspects:
        y_true = np.array(true_labels[aspect])
        y_pred = np.array(predicted_labels[aspect])
        aspect_metrics[aspect] = {
            "accuracy":  accuracy_score(y_true, y_pred),
            "precision": precision_score(y_true, y_pred, average="macro", zero_division=0),
            "recall":    recall_score(y_true, y_pred, average="macro", zero_division=0),
            "f1":        f1_score(y_true, y_pred, average="macro", zero_division=0),
        }

    all_true = np.concatenate([np.array(true_labels[aspect]) for aspect in aspects])
    all_pred = np.concatenate([np.array(predicted_labels[aspect]) for aspect in aspects])
    overall_metrics = {
        "accuracy":  accuracy_score(all_true, all_pred),
        "precision": precision_score(all_true, all_pred, average="macro", zero_division=0),
        "recall":    recall_score(all_true, all_pred, average="macro", zero_division=0),
        "f1":        f1_score(all_true, all_pred, average="macro", zero_division=0),
    }
    return aspect_metrics, overall_metrics


from collections import defaultdict

skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

fold_overall_metrics = []                         # foldÎ≥Ñ Ï†ÑÏ≤¥ ÏßÄÌëú Í∏∞Î°ù
fold_aspectwise_metrics = []                      # foldÎ≥Ñ ÏÜçÏÑ±Î≥Ñ ÏßÄÌëú Í∏∞Î°ù (Î¶¨Ïä§Ìä∏Ïóê dictÎ°ú Ï†ÄÏû•)

for fold, (train_indices, valid_indices) in enumerate(skf.split(train_valid_df, train_valid_df["Îßõ"])):
    print(f"\nüü¢ Fold {fold+1}")

    train_df = train_valid_df.iloc[train_indices]
    valid_df = train_valid_df.iloc[valid_indices]

    train_dataset = MultiAspectDataset(train_df, tokenizer, TEXT_COL, ASPECT_COLS, LABEL2ID)
    valid_dataset = MultiAspectDataset(valid_df, tokenizer, TEXT_COL, ASPECT_COLS, LABEL2ID)
    test_dataset  = MultiAspectDataset(test_df,  tokenizer, TEXT_COL, ASPECT_COLS, LABEL2ID)

    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)
    test_loader  = DataLoader(test_dataset,  batch_size=8, shuffle=False)


    # Î™®Îç∏ Ï¥àÍ∏∞Ìôî
    model = MultiAspectClassifier(model_name=MODEL_NAME, aspects=ASPECT_COLS).to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

    # ===== ÌïôÏäµ =====
    EPOCHS = 3
    for epoch in range(EPOCHS):
        model.train()
        epoch_total_loss = 0.0
        for tokenized_input, labels in train_loader:
            tokenized_input = {key: value.to(device) for key, value in tokenized_input.items()}
            labels = {aspect: label.to(device) for aspect, label in labels.items()}

            optimizer.zero_grad()
            _, loss = model(**tokenized_input, labels=labels)
            loss.backward()
            optimizer.step()

            epoch_total_loss += loss.item()
        print(f"  Epoch {epoch+1}/{EPOCHS} | Train Loss = {epoch_total_loss/len(train_loader):.4f}")

    aspect_metrics, overall_metrics = evaluate_aspectwise(model, valid_loader, ASPECT_COLS)

    print("  ‚ñ∏ Aspect-wise:")
    for aspect in ASPECT_COLS:
        metrics = aspect_metrics[aspect]
        print(f"    [{aspect}] Acc {metrics['accuracy']:.3f} | Prec {metrics['precision']:.3f} | Rec {metrics['recall']:.3f} | F1 {metrics['f1']:.3f}")
    print(f"  ‚ñ∏ Overall : Acc {overall_metrics['accuracy']:.3f} | Prec {overall_metrics['precision']:.3f} | Rec {overall_metrics['recall']:.3f} | F1 {overall_metrics['f1']:.3f}")

    fold_aspectwise_metrics.append(aspect_metrics)
    fold_overall_metrics.append(overall_metrics)

def mean_metric(dict_list, key):
    return float(np.mean([d[key] for d in dict_list]))

overall_mean_metrics = {
    "accuracy":  mean_metric(fold_overall_metrics, "accuracy"),
    "precision": mean_metric(fold_overall_metrics, "precision"),
    "recall":    mean_metric(fold_overall_metrics, "recall"),
    "f1":        mean_metric(fold_overall_metrics, "f1"),
}

print("\n‚≠ê K-Fold ÌèâÍ∑† (Overall)")
print(f"   Acc {overall_mean_metrics['accuracy']:.3f} | Prec {overall_mean_metrics['precision']:.3f} | Rec {overall_mean_metrics['recall']:.3f} | F1 {overall_mean_metrics['f1']:.3f}")

sample_text = "Í∞ÄÍ≤©ÏùÄ Í¥úÏ∞ÆÏßÄÎßå ÏÑúÎπÑÏä§Í∞Ä Î≥ÑÎ°úÏòÄÏñ¥Ïöî."
tokenized_sample = tokenizer(sample_text, return_tensors='pt', truncation=True, padding='max_length', max_length=128).to(device)
logits, _ = model(**tokenized_sample)
for aspect in ASPECT_COLS:
    predicted_label_id = torch.argmax(logits[aspect], dim=-1).item()
    print(f"{aspect}: {ID2LABEL[predicted_label_id]}")





def predict_and_save(model, df, tokenizer, text_col, aspects, id2label, out_path):
    model.eval()
    predictions_by_aspect = {aspect: [] for aspect in aspects}
    with torch.no_grad():
        for text in df[text_col]:
            tokenized_input = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=128).to(device)
            logits, _ = model(**tokenized_input)
            for aspect in aspects:
                predicted_label_id = torch.argmax(logits[aspect], dim=-1).item()
                predictions_by_aspect[aspect].append(id2label[predicted_label_id])

        aspect_name_map = {
            "Í∞ÄÍ≤©": "price",
            "ÏÑúÎπÑÏä§": "service",
            "Îßõ": "taste",
            "Î∂ÑÏúÑÍ∏∞": "ambience"  # ambience(Î∂ÑÏúÑÍ∏∞) ÎòêÎäî atmosphereÎ°ú Ïç®ÎèÑ Î¨¥Î∞©
        }

    for aspect in aspects:
        english_aspect_name = aspect_name_map[aspect]
        df[f"{english_aspect_name}_pred"] = predictions_by_aspect[aspect]


    df.to_csv(out_path, index=False, encoding='utf-8-sig')
    print(f"‚úÖ Í≤∞Í≥º Ï†ÄÏû• ÏôÑÎ£å: {out_path}")

predict_and_save(model, test_df, tokenizer, TEXT_COL, ASPECT_COLS, ID2LABEL,
                 "/content/test.csv")

# /content/drive/MyDrive/Ehwa/rieviews_sentence_final.csv

unlabeled_review_df = pd.read_csv("/content/drive/MyDrive/Ehwa/review_sentence_v7.csv")

def normalize_text(text):
    if pd.isna(text):
        return ""
    if isinstance(text, list):
        return " ".join(map(str, text))
    return str(text)

unlabeled_review_df["joined_sentence"] = unlabeled_review_df["joined_sentence"].map(normalize_text)

unlabeled_review_df.head()

predict_and_save(
    model,
    unlabeled_review_df,
    tokenizer,
    text_col="joined_sentence",
    aspects=ASPECT_COLS,
    id2label=ID2LABEL,
    out_path="/content/drive/MyDrive/Ehwa/new_predictions.csv"
)

import os
save_path = "/content/drive/MyDrive/Ehwa/final_Bert_MLP_model.pt"

# Î™®Îç∏ Ï†ÄÏû•
torch.save(model.state_dict(), save_path)
print(f"‚úÖ Î™®Îç∏ Ï†ÄÏû• ÏôÑÎ£å: {save_path}")


