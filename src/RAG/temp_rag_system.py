# -*- coding: utf-8 -*-
"""캡스톤디자인_RAG_V5_종합.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TAjXqqJvOTE-tXOPuwR88nRGdyPjWg5t

벡터DB 업로드
"""

from google.colab import drive
drive.mount('/content/drive')

# !pip install -U langchain langchain-chroma
# !pip install -U langchain-huggingface sentence-transformers

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# Chroma 데이터베이스 경로
chroma_path = "/content/drive/MyDrive/local_chroma/review_db_v13"

# 임베딩 모델
embedding_model = HuggingFaceEmbeddings(model_name="BM-K/KoSimCSE-roberta-multitask")

# Chroma 벡터DB 불러오기
vector_db = Chroma(
    collection_name="review_collection",
    embedding_function=embedding_model,
    persist_directory=chroma_path
)

# 공식 API로 문서 수 확인
count = vector_db._collection.count()  # 내부 client 대신 collection 속성 사용
print(f"복원된 문서 수: {count}")

vector_db_data = vector_db.get()

print(vector_db_data.keys())

documents = vector_db.get()['documents']
print(documents[:5])

"""rag 예시"""

import os
# API 키는 환경 변수에서 읽어옵니다
# export OPENAI_API_KEY="your-api-key" 또는 .env 파일 사용
if "OPENAI_API_KEY" not in os.environ:
    raise ValueError("OPENAI_API_KEY 환경 변수가 설정되지 않았습니다.")

"""JSON 저장 METADATA 불러오기"""

import json
import pandas as pd

# JSON 메타데이터 로드
metadata_file_path = "/content/drive/MyDrive/ABSA_results/restaurant_metadata.json"

with open(metadata_file_path, "r", encoding="utf-8") as f:
    restaurant_metadata_list = json.load(f)

restaurant_metadata_df = pd.DataFrame(restaurant_metadata_list)
print(f"JSON 메타데이터 로드 완료 ({len(restaurant_metadata_df)}개)")
display(restaurant_metadata_df.head(3))

# Chroma 벡터DB에서 메타데이터 추출
vector_db_items = vector_db.get(include=["metadatas", "documents"])

# dict → DataFrame 변환
vector_db_metadata_df = pd.DataFrame(vector_db_items["metadatas"])
vector_db_metadata_df["page_content"] = vector_db_items["documents"]

print(f"Chroma 내 메타데이터 추출 완료 ({len(vector_db_metadata_df)}개)")
display(vector_db_metadata_df.head(3))

# restaurant_id 기준 병합
merged_restaurant_df = pd.merge(
    vector_db_metadata_df,
    restaurant_metadata_df,
    on="restaurant_id",
    how="left",
    suffixes=("_db", "_json")
)

print(f"병합 완료: {len(merged_restaurant_df)}개 매칭")
display(merged_restaurant_df[["restaurant_id", "restaurant_name_db", "review_no"]].head(5))

# 병합 실패(매칭 안 된) 식당 확인
unmatched_restaurants = merged_restaurant_df[merged_restaurant_df["restaurant_name_json"].isna()]
print(f"매칭 안 된 식당 수: {len(unmatched_restaurants)}")

if len(unmatched_restaurants) > 0:
    display(unmatched_restaurants.head(3))

# 매칭 실패(restaurant_name_json이 NaN)인 행 제거
before_count = len(merged_restaurant_df)
merged_restaurant_df = merged_restaurant_df[merged_restaurant_df["restaurant_name_json"].notna()].reset_index(drop=True)
after_count = len(merged_restaurant_df)

print(f" 매칭 안 된 식당 {before_count - after_count}개 삭제 완료")
print(f"남은 데이터 수: {after_count}개")

print("vector_db_metadata_df 컬럼 목록:\n", vector_db_metadata_df.columns.tolist())
print("\n restaurant_metadata_df 컬럼 목록:\n", restaurant_metadata_df.columns.tolist())
print("\n merged_restaurant_df 컬럼 목록:\n", merged_restaurant_df.columns.tolist())

"""chroma와 json의 data가 중복된 경우 하나 삭제
- restaurant_name_json, category_1_json, category_2_json, and station_json 삭제 가능
- 나중에 필요하면 적용
"""

# 삭제할 열 목록 정의
duplicate_columns_to_drop = [
    "restaurant_name_json",
    "category_1_json",
    "category_2_json",
    "station_json"
]

# 실제로 존재하는 열만 삭제 (오류 방지용)
duplicate_columns_to_drop = [col for col in duplicate_columns_to_drop if col in merged_restaurant_df.columns]
merged_restaurant_df = merged_restaurant_df.drop(columns=duplicate_columns_to_drop)

print(f"불필요한 JSON 열 {len(duplicate_columns_to_drop)}개 삭제 완료")
print("남은 열 예시:")
display(merged_restaurant_df.head(3))

print("\n merged_restaurant_df 컬럼 목록:\n", merged_restaurant_df.columns.tolist())

merged_restaurant_df["category_1_db"].unique()

"""dat"""

merged_restaurant_df["date"].unique()

merged_restaurant_df["raw"].unique()

"""##1차필터"""

# !apt-get install openjdk-11-jdk -y

import os

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["PATH"] += os.pathsep + os.path.join(os.environ["JAVA_HOME"], "bin")

# !pip install konlpy

from konlpy.tag import Okt
import re

okt_tokenizer = Okt()

def preprocess_query(text):
    text = re.sub(r"[^가-힣a-zA-Z0-9\s]", " ", text)
    tokens = okt_tokenizer.pos(text, stem=True)
    keywords = [w for w, p in tokens if p in ["Noun", "Adjective", "Verb"]]
    stopwords = ['에서', '으로', '은', '는', '이', '가', '을', '를', '의', '도', '에', '로', '과', '와', '고', '들', '좀', '처럼', '도', '한']
    keywords = [k for k in keywords if k not in stopwords]
    return keywords

def build_filters(df, keywords, verbose=False):
    filter_condition = pd.Series(True, index=df.index)

    # ============================================================
    # 역 필터 (동의어 기반)
    # ============================================================
    if "station_db" in df.columns:

        station_synonyms = {
            "성수": ["성수", "성수역"],
            "서울숲": ["서울숲", "서울숲역"],
            "뚝섬": ["뚝섬", "뚝섬역"],
            "자양": ["자양", "자양동"],
            "건대입구": ["건대입구", "건대", "건대근처", "건대역", "건국대"],
            "어린이대공원": ["어린이대공원", "어린이대공원역", "어대공", "대공원"],
        }

        matched_station_names = []

        # 사용자가 언급한 역 이름들을 추출
        for standard_station, syns in station_synonyms.items():
            if any(syn in keywords for syn in syns):
                matched_station_names.append(standard_station)

        # 역이 여러 개 매칭된 경우 → OR 조건으로 묶어야 정상
        if len(matched_station_names) > 0:
            station_filter_condition = pd.Series(False, index=df.index)
            for station_name in matched_station_names:
                station_filter_condition |= df["station_db"].astype(str).str.contains(station_name, na=False)

            filter_condition &= station_filter_condition


    # ============================================================
    # 시간대 필터
    # ============================================================
    lunch_time_columns = [col for col in df.columns if col.endswith("_lunch")]
    dinner_time_columns = [col for col in df.columns if col.endswith("_dinner")]

    if any(kw in keywords for kw in ["점심", "브런치", "아점", "런치"]):
        filter_condition &= df[lunch_time_columns].any(axis=1)

    if any(kw in keywords for kw in ["저녁", "야식", "디너", "심야"]):
        filter_condition &= df[dinner_time_columns].any(axis=1)


    # ============================================================
    # 카테고리 필터
    # ============================================================
    if "category_1_db" in df.columns:
        df["category_1_db"] = df["category_1_db"].astype(str).str.strip()

        category_synonyms = {
            "한식": ["한식", "백반", "국밥", "삼겹살", "목살", "칼국수", "보쌈", "한정식"],
            "일식": ["일식", "스시", "초밥", "라멘", "덮밥", "가츠동", "오마카세", "오니기리", "규동"],
            "중식": ["중식", "짜장", "짬뽕", "탕수육", "훠궈", "마라샹궈","마라탕", "양꼬치"],
            "양식": ["양식", "파스타", "스테이크", "브런치"],
            "술집": ["술집", "바", "이자카야", "펍", "포차", "포장마차"],
            "간식": ["간식", "카페", "디저트", "빵집", "베이글", "브런치"],
            "분식": ["분식", "떡볶이", "즉떡", "김밥", "라면"],
        }

        simple_categories = [
            "구내식당", "치킨",
            "패스트푸드", "퓨전요리", "도시락",
            "아시아음식", "샤브샤브", "뷔페",
            "식품판매", "샐러드", "패밀리레스토랑"
        ]

        if isinstance(keywords, str):
            keywords = [keywords]

        # --- 카테고리 키워드 사용 여부 판단
        all_cats = list(category_synonyms.keys()) + simple_categories
        category_keyword_used = any(
            any(syn in " ".join(keywords) for syn in syns)
            for syns in category_synonyms.values()
        ) or any(cat in " ".join(keywords) for cat in simple_categories)

        # --- 카테고리 키워드가 없다면 필터 비활성화
        if category_keyword_used:
            category_matched = False

            # 1) 동의어 기반 매칭
            for cat, syns in category_synonyms.items():
                if any(any(syn in kw for kw in keywords) for syn in syns):
                    filter_condition &= df["category_1_db"].str.contains(cat, case=False, na=False)
                    category_matched = True
                    break

            # 2) 단순 카테고리명 매칭
            if not category_matched:
                for cat in simple_categories:
                    if any(cat in kw for kw in keywords):
                        filter_condition &= df["category_1_db"].str.contains(cat, case=False, na=False)
                        category_matched = True
                        break

            # 3) 매칭 실패 시 전체 포함 → 아무것도 하지 않음
            if not category_matched:
                pass


    # ============================================================
    # 점수 기반 필터 (substring 기반)
    # ============================================================
    FILTER_KEYWORDS = {
        "service_score": ["친절", "응대", "서비스", "빠른"],
        "ambience_score": ["분위기", "감성", "데이트", "기념일"],
        "price_score": ["저렴", "가성비", "합리적"],
        "taste_score": ["맛있", "맛집", "훌륭"],
    }

    for col, kw_list in FILTER_KEYWORDS.items():
        if col in df.columns:
            # substring 기반 score 필터 발동
            if any(
                any(score_kw in kw for kw in keywords)
                for score_kw in kw_list
            ):
                filter_condition &= df[col] >= 0.5


    # ============================================================
    # 결과 반환
    # ============================================================
    filtered_reviews_df = df[filter_condition].copy()

    if verbose:
        print(f"필터링 완료: {len(filtered_reviews_df)}개 리뷰 / {filtered_reviews_df['restaurant_name_db'].nunique()}개 식당")

    return filtered_reviews_df

"""#2차 임베딩 버전1 : 쿼리와 리뷰데이터간의 유사도 점수 계산, retreiver"""

import numpy as np
import pandas as pd
from numpy.linalg import norm

def build_sim_and_top_restaurants(query, merged_restaurant_df, vector_db, embedding_model,
                                  top_k_rest=3, reviews_per_rest=10,
                                  lambda_weight=0.0001):

    print("\n==============================")
    print(f"[쿼리] {query}")
    print("==============================\n")

    # ------------------------------------------------
    # 1) 1차 필터링
    # ------------------------------------------------
    filtered_df = build_filters(merged_restaurant_df.copy(), query)
    filtered_review_ids = filtered_df["review_no"].astype(int).tolist()

    print(f"[1차 필터] 리뷰 개수: {len(filtered_review_ids)}")

    if len(filtered_review_ids) == 0:
        return pd.DataFrame(), pd.DataFrame()

    # ------------------------------------------------
    # 2) 쿼리 임베딩
    # ------------------------------------------------
    query_embedding = embedding_model.embed_query(query)

    # ------------------------------------------------
    # 3) 벡터DB review subset 로드
    # ------------------------------------------------
    filtered_vector_subset = vector_db._collection.get(
        where={"review_no": {"$in": filtered_review_ids}},
        include=["embeddings", "metadatas"]
    )

    print(f"[벡터DB] 임베딩 로드 개수: {len(filtered_vector_subset['embeddings'])}")

    if len(filtered_vector_subset["embeddings"]) == 0:
        return pd.DataFrame(), pd.DataFrame()

    # ------------------------------------------------
    # 4) 유사도 계산
    # ------------------------------------------------
    review_embedding_vectors = np.array(filtered_vector_subset["embeddings"])
    similarity_scores = np.dot(review_embedding_vectors, query_embedding) / (
        norm(review_embedding_vectors, axis=1) * norm(query_embedding)
    )

    similarity_df = pd.DataFrame({
        "review_no": [meta["review_no"] for meta in filtered_vector_subset["metadatas"]],
        "restaurant_id": [meta["restaurant_id"] for meta in filtered_vector_subset["metadatas"]],
        "restaurant_name": [meta["restaurant_name"] for meta in filtered_vector_subset["metadatas"]],
        "date": [meta["date"] for meta in filtered_vector_subset["metadatas"]],
        "similarity": similarity_scores,
    })

    # ------------------------------------------------
    # 5) 날짜 처리 + 시간 가중치
    # !! 최신리뷰에 가중치 !!
    # ------------------------------------------------
    similarity_df["date"] = pd.to_datetime(similarity_df["date"], format="mixed")
    latest_review_date = similarity_df["date"].max()
    similarity_df["days_since"] = (latest_review_date - similarity_df["date"]).dt.days

    similarity_df["time_weight"] = np.exp(-lambda_weight * similarity_df["days_since"])
    similarity_df["weighted_similarity"] = similarity_df["similarity"] * similarity_df["time_weight"]

    # ------------------------------------------------
    # 6) 식당별 TOP N 리뷰 평균해서 TOP K 식당 뽑기
    # 식당별 group by
    # ------------------------------------------------
    restaurant_aggregated_df = (
        similarity_df.sort_values(["restaurant_id", "weighted_similarity"], ascending=[True, False])
        .groupby("restaurant_id")
        .head(reviews_per_rest)
        .groupby(["restaurant_id", "restaurant_name"], as_index=False)["weighted_similarity"]
        .mean()
        .sort_values("weighted_similarity", ascending=False)
    )

    top_recommended_restaurants = restaurant_aggregated_df.head(top_k_rest)

    print("\n[TOP 추천 식당]")
    print(top_recommended_restaurants)

    return similarity_df, top_recommended_restaurants

"""LLM으로 보낼 정보 수집
 - 선정 근거 리뷰3개 + MERGED_DF에 저장되어있는 식당 정보

"""

def build_context_from_top_restaurants(top_recommended_restaurants, similarity_df, merged_restaurant_df, vector_db, top_k_review=3):

    restaurant_context_list = []

    for _, restaurant_row in top_recommended_restaurants.iterrows():
        restaurant_id = restaurant_row["restaurant_id"]
        restaurant_name = restaurant_row["restaurant_name"]

        # 1) 식당 메타데이터
        restaurant_metadata = merged_restaurant_df[merged_restaurant_df["restaurant_id"] == restaurant_id].iloc[0].to_dict()

        # 2) similarity_df 기반 상위 리뷰 TOP3 추출
        top_reviews_for_restaurant_df = (
            similarity_df[similarity_df["restaurant_id"] == restaurant_id]
            .sort_values("weighted_similarity", ascending=False)
            .head(top_k_review)
        )

        # 3) 리뷰 텍스트는 similarity_df가 아니라 Chroma에서 review_no로 가져오기
        top_review_ids = top_reviews_for_restaurant_df["review_no"].astype(int).tolist()

        review_subset = vector_db._collection.get(
            where={"review_no": {"$in": top_review_ids}},
            include=["documents", "metadatas"]
        )

        # 4) 리뷰 리스트 구성 (text는 Chroma documents에서 정확히 해당 리뷰만)
        top_reviews_list = []
        for document, review_metadata in zip(review_subset["documents"], review_subset["metadatas"]):

            # similarity_df에서 계산된 similarity 가져오기
            similarity_score = float(
                top_reviews_for_restaurant_df[top_reviews_for_restaurant_df["review_no"] == review_metadata["review_no"]]["weighted_similarity"].values[0]
            )

            top_reviews_list.append({
                "review_no": review_metadata["review_no"],
                "date": str(review_metadata["date"]),
                "similarity": similarity_score,
                "text": document
            })

        # 5) context에 저장
        restaurant_context_list.append({
            "restaurant_id": restaurant_id,
            "restaurant_name": restaurant_name,
            "meta": restaurant_metadata,
            "top_reviews": top_reviews_list
        })

    return {"restaurants": restaurant_context_list}

# def convert_df_to_json(df, columns, max_rows=20):
#     df = df[columns].head(max_rows)
#     return df.to_dict(orient="records")

# !pip install -q langchain langchain-core langchain-community

# !pip install -q langchain-openai

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.2
    # API 키는 환경 변수에서 자동으로 읽어옵니다
)

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage
import json

prompt = ChatPromptTemplate.from_messages([
    ("system",
    """당신은 서울 성수동, 서울숲, 건대 지역의 식당을 추천하는 전문 챗봇입니다.

[응답 원칙]

1. 반드시 제공된 JSON 데이터(context)에 있는 사실만 사용해야 합니다.
   - JSON 필드가 없거나 해당 정보가 비어 있으면 그 항목은 언급하지 않습니다.

2. JSON 데이터에서 식당 정보를 전혀 찾을 수 없을 경우
   아래 문장을 그대로 출력하세요:
   "관련된 정보가 없습니다."

3. '처음 사용자 질문'일 경우, 아래 형식에 따라 JSON 안의 서로 다른 식당 총 3곳을 추천하세요.
   - 추천 식당명
   - 추천 메뉴 (여러 메뉴가 있을 경우 한 번에 나열)
   - Best review : top_reviews 리스트에서 similarity가 가장 높은 리뷰 하나를 선택해,
      해당 리뷰의 "text" 값을 원문 그대로 인용하세요.
      (절대 요약·의역·변형 금지. 띄어쓰기, 맞춤법 오류까지 그대로 유지할 것)
      인용 형식: "<리뷰 text 원문>"
   - 추천 이유 : JSON에 있는 구조적 정보와 리뷰 전체 분위기를 기반으로 자연스럽게 작성
     (주의: 리뷰 원문을 다시 인용하거나 요약한 문장을 넣지 말 것)

4. '후속 질문'일 경우에는 위의 추천 형식을 사용하지 말고,
   질문 의도에 맞는 자연스러운 답변만 제공합니다.
   (리뷰를 인용할 필요도 없음)

5. '처음 질문'과 '후속 질문'의 구분 기준
   - chat_history가 비어 있으면 '처음 질문'으로 간주
   - chat_history에 내용이 있다면 '후속 질문'으로 간주
    """
    ),

    MessagesPlaceholder(variable_name="chat_history"),

    ("human",
    """다음은 JSON 컨텍스트 데이터입니다:
{context}

사용자 질문:
{question}
    """)
])

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)

rag_chain = prompt | llm

def fix_timestamps(data_object):
    """Timestamp가 들어 있는 dict/list 구조를 재귀적으로 모두 문자열로 변환"""
    if isinstance(data_object, dict):
        converted_object = {}
        for key, value in data_object.items():
            if isinstance(value, pd.Timestamp):
                converted_object[key] = value.isoformat()
            else:
                converted_object[key] = fix_timestamps(value)
        return converted_object

    elif isinstance(data_object, list):
        return [fix_timestamps(item) for item in data_object]

    else:
        return data_object

def run_full_pipeline(query, merged_restaurant_df, vector_db, embedding_model, chat_history):
    """
    query 1개 넣으면:
    1) 유사도 기반 sim_df 생성
    2) TOP3 식당 선정
    3) context JSON 구성
    4) LLM 실행
    5) chat_history 업데이트
    → 모든 것을 한 번에 처리
    """

    print(f"\n===== [STEP 1] 검색 및 유사도 계산 =====")
    similarity_df, top_recommended_restaurants = build_sim_and_top_restaurants(
        query=query,
        merged_restaurant_df=merged_restaurant_df,
        vector_db=vector_db,
        embedding_model=embedding_model,
        top_k_rest=3,
        reviews_per_rest=10
    )

    if len(top_recommended_restaurants) == 0:
        print("검색된 식당이 없습니다.")
        return chat_history

    print("\n>>> 선정된 TOP3 식당:")
    display(top_recommended_restaurants)

    print(f"\n===== [STEP 2] 컨텍스트 JSON 구성 =====")
    context_data = build_context_from_top_restaurants(
        similarity_df=similarity_df,
        top_recommended_restaurants=top_recommended_restaurants,
        merged_restaurant_df=merged_restaurant_df,
        vector_db=vector_db,
        top_k_review=3
    )

    # JSON 변환 가능한 형태로 변환
    context_data = fix_timestamps(context_data)

    print("\n>>> LLM에 전달되는 JSON 예시:")
    print(json.dumps(context_data, ensure_ascii=False, indent=2))

    print(f"\n===== [STEP 3] LLM 응답 생성 =====")
    llm_response = rag_chain.invoke({
        "context": json.dumps(context_data, ensure_ascii=False, indent=2),
        "question": query,
        "chat_history": chat_history
    })

    print("\n\n=== LLM 첫 질문 응답 ===\n")
    print(llm_response.content)

    # chat_history 업데이트
    chat_history.append(HumanMessage(content=query))
    chat_history.append(AIMessage(content=llm_response.content))

    return context_data, chat_history

"""후속질문용 함수"""

def follow_up_pipeline(query, context_data, chat_history):

    llm_response = rag_chain.invoke({
        "context": json.dumps(context_data, ensure_ascii=False, indent=2),
        "question": query,
        "chat_history": chat_history
    })

    print("\n=== 후속 질문 응답 ===")
    print(llm_response.content)

    chat_history.append(HumanMessage(content=query))
    chat_history.append(AIMessage(content=llm_response.content))

    return chat_history

"""예시1"""

chat_history = []

context_data, chat_history = run_full_pipeline(
    query="서울숲에서 데이트하려고 하는데 카페 추천해줘",
    merged_restaurant_df=merged_restaurant_df,
    vector_db=vector_db,
    embedding_model=embedding_model,
    chat_history=chat_history
)

#후속질문은 follow_up_pipeline 함수
chat_history = follow_up_pipeline(
    query="추천한 식당 중에서 분위기가 가장 좋은 한곳만 추천해줘",
    context_data=context_data,
    chat_history=chat_history
)

#후속질문은 follow_up_pipeline 함수
chat_history = follow_up_pipeline(
    query="그 식당들 가려면 지하철역에서 얼마나 걸리나?",
    context_data=context_data,
    chat_history=chat_history
)

#후속질문은 follow_up_pipeline 함수
chat_history = follow_up_pipeline(
    query="퍼먼트랑 한입베이글의 음료와 빵 가격대가 어떻게 되나? 너무 비싸면 부담스러운데",
    context_data=context_data,
    chat_history=chat_history
)

#후속질문은 follow_up_pipeline 함수
chat_history = follow_up_pipeline(
    query="퍼먼트 영업시간 알려줘",
    context_data=context_data,
    chat_history=chat_history
)

#후속질문은 follow_up_pipeline 함수 --> 새로운 리스트를 얻고자 하면 함수를 바꿔야 함
chat_history = follow_up_pipeline(
    query="이미 언급한 3군데 말고 다른 디저트 가게도 알려줘",
    context_data=context_data,
    chat_history=chat_history
)

"""예시2"""

chat_history = []

context_data, chat_history = run_full_pipeline(
    query="건대에서 저녁 회식을 하려고해. 한식, 양식 위주로 추천해줘",
    merged_restaurant_df=merged_restaurant_df,
    vector_db=vector_db,
    embedding_model=embedding_model,
    chat_history=chat_history
)

#후속질문은 follow_up_pipeline 함수
chat_history = follow_up_pipeline(
    query="추천 식당들 중에 매콤한 메뉴가 있나?",
    context_data=context_data,
    chat_history=chat_history
)

#후속질문은 follow_up_pipeline 함수
chat_history = follow_up_pipeline(
    query="혹시 그중에 비건 메뉴가 있는 식당이 있나?",
    context_data=context_data,
    chat_history=chat_history
)

#후속질문은 follow_up_pipeline 함수
chat_history = follow_up_pipeline(
    query="모임 예산을 측정해야 해서 대략적 가격도 알려줘",
    context_data=context_data,
    chat_history=chat_history
)

#후속질문은 follow_up_pipeline 함수
chat_history = follow_up_pipeline(
    query="육일관 전화번호 알 수 있나?",
    context_data=context_data,
    chat_history=chat_history
)

#후속질문은 follow_up_pipeline 함수
chat_history = follow_up_pipeline(
    query="회식이다보니 술을 마실텐데 안주 메뉴는?",
    context_data=context_data,
    chat_history=chat_history
)

"""예시3

"""

chat_history = []

context_data, chat_history = run_full_pipeline(
    query="서울숲과 건대역에 일식 맛집 추천해줘",
    merged_restaurant_df=merged_restaurant_df,
    vector_db=vector_db,
    embedding_model=embedding_model,
    chat_history=chat_history
)

#후속질문은 follow_up_pipeline 함수
chat_history = follow_up_pipeline(
    query="코시랑 도우또 지하철역에서 가는 방법 알려줘",
    context_data=context_data,
    chat_history=chat_history
)

"""#MMR 방법 적용

"""

